
\documentclass{article}

\title{An Exact Method of Solving PDEs}
\author{Brent Baccala\footnote{Email address: \tt cosine@freesoft.org}}

\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{mathtools}  % for /xRightarrow

\usepackage{xcolor}
\usepackage{comment}

\usepackage[hidelinks]{hyperref}

\usepackage{tabularx}

\usepackage{longtable}

% For drawing ansatz diagrams

\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{positioning}
\usetikzlibrary{fit}
\usetikzlibrary{backgrounds}
\usetikzlibrary{shapes.multipart}

\def\coeff{\framebox(10,10){}}
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}
\def\R32003{$F_{32003}$}

\begin{document}
\parindent 0pt

\maketitle

\begin{abstract}
The author has developed an algorithm, based on differential algebra,
for finding exact, non-separable solutions of partial differential equations.
\end{abstract}

\parskip 12pt

% \subsection*{Introduction}

% An January 2023, I discovered a previously unknown solution to the simplest Schrödinger equation for the hydrogen atom.
%
% It turns out that this wavefunction:
%
% \begin{equation}
% \Psi = J_0(2\sqrt{x+r})
% \end{equation}
%
% where $J_0$ is the ordinary Bessel function $J_0$, solves the Schrödinger equation for hydrogen:
%
% \begin{equation}
% -\frac{1}{2}\nabla^2 \Psi - \frac{1}{r}\Psi = E \Psi
% \end{equation}
%
% with E=0.
%
% This paper explains the solution technique, which is generally applicable to all PDEs.
%
% %\begin{comment}
% A short list of methods to find exact solutions to PDEs:
%
% \begin{itemize}
% \item separation of variables
%
% Assume that the solution is a product of simpler functions, each of which depends on a subset of the variables
% \item method of characteristics
% \item transform methods (Fourier transform on space variables)
% \item symmetry methods
% \item calculus of variations
% \item Evans: Laplace's eq is invariant under rotations, so look for functions of r; this is a variant of sep of var
% \item Evans: Poisson's eq: Green's function is constructed from radially symmetric solution to Laplace's eq
% \item Evans: Poisson's eq: calculus of variations; show that solution minimizes a functional
% \item Evans: heat eq: uses several scaling symmetries to justify solution form depending on a single expression
% \item Evans: Duhamel's principle: separate out one variable (t) that appears like $u_t - Lu = f$ (L has no time deriv);
%       form the ``retarded solution'' that represents the effect of an infinitesimal $f$, then integrate over time
% \item Evans: heat eq: scaling symmetry $\rightarrow$ fundamental sol $\rightarrow$ convolution to handle arbitrary initial condition
%       $\rightarrow$ Duhamel's principle to handle the inhomogenous component
% \end{itemize}
% %\end{comment}
%
% \begin{comment}
% A short list of methods to find exact solutions to PDEs includes separation of variables,
% the method of characteristics, transform methods (including Fourier transforms),
% symmetry methods, Green's functions, Duhamel's principle, and the calculus of variations.
% The author has developed another exact solution technique based on differential algebra
% and has used it to find a new solution to one of the most well-studied equations
% in mathematical physics, the Schr\"odinger equation for hydrogen.
% \end{comment}

\section{Introduction}
%% \subsection*{Differential Algebra}

We can construct polynomial rings and fields using indeterminates, such as $\mathbb{Q}[x,y]$,
treating the indeterminates $x$ and $y$ merely as elements of the ring that satisfy
its basic ring axioms.  In this paper, all rings will be commutative, with a unity element,
and of characteristic zero. We call this ``commutative algebra''.

We can then consider mapping the indeterminants into the coefficient field, and ask questions like,
``Which values of $x$ and $y$ in $\mathbb{Q}$ satisfy $x^2+y^2=4$?''  We can consider systems of polynomial
equations, and seek to describe which values of indeterminates satisfy all of the equations
in the system.  We introduce ideals in the polynomial ring and algebraic varieties in the
solution space and deduce the contravariant equivalence between them (\cite{iva} \S 4.2)).  We call this
``algebraic geometry''.  See Figure \ref{concept figure}.

\begin{figure}
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[node distance=50pt, every text node part/.style={align=center}]
\node (commutative algebra) [draw, text width=200pt] {Commutative algebra
      \begin{itemize}
        \item rings, ideals, fields, algebraic extensions
        \item indeterminates are simply objects that obey the commutative, associative, distributive axioms
        \item Ex: $x^2+y^2+z^2=1$
      \end{itemize}
};
\node (algebraic geometry) [draw, node distance=100pt, below of=commutative algebra, text width=200pt] {Algebraic Geometry
      \begin{itemize}
        \item indeterminates now take values in a field like $\mathbb{C}, \mathbb{R}, \mathbb{Q}, \overline{\mathbb{Q}}, \mathbb{F}_p$
        \item Ex: surface of unit sphere in $\mathbb{R}^3$
      \end{itemize}
};
\node (differential algebra) [draw, node distance=225pt, right of=commutative algebra, text width=200pt] {Differential algebra
      \begin{itemize}
        \item a third primitive operator (derivation) is added to addition and multiplication
        \item differential rings, differential ideals, differential transcendental extensions
        \item Ex: $\Psi_{xx}=\Psi_{t}$
      \end{itemize}
};
\node (differential algebraic geometry) [draw, node distance=100pt, below of=differential algebra, text width=200pt] {Differential Algebraic Geometry
      \begin{itemize}
        \item indeterminates now take values in function spaces like $C^2(U)$, $H^2(\mathbb{R}^3)$
        \item Ex: $\Psi = e^{-x^2/4t}/\sqrt{t}$
      \end{itemize}
};

\draw [very thick, ->] (commutative algebra.south) -- (algebraic geometry.north);
\draw [very thick, ->] (differential algebra.south) -- (differential algebraic geometry.north);

\draw [very thick, ->] (differential algebraic geometry.south) to[out=-135, in=-45] coordinate[midway] (mid) (algebraic geometry.south) ;

\node [below of=mid, node distance=20pt] {\bf We can do this\\\bf if we use function spaces parameterized with constants};

\end{tikzpicture}%
}
\caption{The interrelation between four relevant branches of mathematics}
\label{concept figure}
\end{figure}

J.F. Ritt introduced differential algebra \cite{Ritt}.
Starting with a ring $R$ or a field $F$, we can construct a
{\it differential ring} or a {\it differential field}
by specifying one or more derivations, unary operators that
% commute with addition and obey the Leibniz rule for multiplication:
obey two additional axioms:

\begin{equation*}
\begin{gathered}
dx(a+b) = dx(a) + dx(b) \\
dx(ab) = (dx(a))b + a(dx(b))
\end{gathered}
\end{equation*}

Various notations are in common use for the derivations;
since we work mostly in a PDE context where there are multiple
derivations, I refer to derivations by prefixing
them with a $d$, i.e, $dx$.  We will further assume that
all derivations are mutually commutative:

\begin{equation*}
dx(dy(a)) = dy(dx(a))
\end{equation*}

Let $\Theta$ be the free abelian monoid generated by the derivations;
we call the elements of $\Theta$ the {\it derivative operators}, and they act on
the differential ring by repeated application of the derivations.
The identity element of $\Theta$ acts as the identity mapping on the differential ring.

A derivative operator applied to an indeterminate is called a {\it derivative}.
I generally use ``jet notation'' for derivatives,
where the applied derivations
are denoted by subscripted letters on the indeterminates, i.e, $\Psi_x$
is the result of applying derivation $dx$ to $\Psi$.

Although we could specify that a derivation maps an indeterminate to some specific
element of the differential ring, it more convenient to
let each indeterminate introduce an infinite set of derivatives by pairing
the indeterminate with all the elements of $\Theta$, so introducing $\Psi$
also introduces $\Psi_x$, $\Psi_y$, $\Psi_{xy}$, $\Psi_{xxx}$, etc.  We can
then specify that $dx$ maps $\Psi$ to an element $e$ either by stating
that $\Psi_x=e$, or taking the differential ring modulo $\Psi_x-e$.

For each derivation, there will typically exist in the differential ring an associated indeterminate,
labeled with the same letter,
with the property that applying a derivation to its
associated indeterminate produces unity, i.e, $x_x = 1$.  Applying a derivation to
the associated indeterminate of a different derivation produces zero, i.e, $y_x = 0$.
Sometimes it is important to remember that while the same letter is used for both
$x$ and $dx$, they are distinct objects. $x$ is an element of a differential
ring, while $dx$ is a unary operator defined on a differential ring.

Analytically, we will always interpret $\Psi_x$ as the derivative
of $\Psi$ with respect to $x$;
thinking of the derivation as a unary operator applied to an indeterminate
is an algebraic concept.  The distinction is much the same
as between the real number $\sqrt{2}$ and the element $\gamma$
in the algebraic extension of $\mathbb{Q}$ defined modulo the ideal $(\gamma^2-2)$.

Ritt's convention was to write differential rings by stating their coefficient
field, then listing the associated indeterminates of derivations enclosed in curly
braces.  Each listed indeterminate corresponds to both a derivation (prefixed by $d$)
and its associated indeterminate.

For most of this paper, we'll be working in the following differential ring:

\newcommand{\ols}[1]{\mskip.5\thinmuskip\overline{\mskip-.5\thinmuskip {#1} \mskip-.5\thinmuskip}\mskip.5\thinmuskip} % overline short
\newcommand{\olsi}[1]{\,\overline{\!{#1}}} % overline short italic

\begin{equation}
\begin{gathered}
%%   R= \mathbb{Q}\{x,y,z\}[E,a_0,...,v_4,r,\Psi,\Psi',\Psi'',v] \mod (r^2-x^2-y^2-z^2) \\
%%   R= \mathbb{Q}\{x,y,z\}[E,a_0,...,v_4,r,\Psi,\Psi',\Psi'',v] \\
%%   {\rm where\ } a_0,...,v_4 = a_0,a_1,b_0,b_1,c_0,c_1,v_1,v_2,v_3,v_4 \\
   R= \mathbb{Q}\{x,y,z\}[E,a_0,a_1,b_0,b_1,c_0,c_1,v_1,v_2,v_3,v_4,r,\Psi,\Psi',\Psi'',v] \\
\end{gathered}
\end{equation}

To elaborate,
\begin{itemize}
   \item $\mathbb{Q}\{x,y,z\}$ (and thus $R$) is a differential ring with derivations $dx$, $dy$ and $dz$ and matching indeterminates $x$, $y$, and $z$
   \item $\mathbb{Q}\{x,y,z\}$ is extended by adjoining sixteen indeterminates with no matching derivations
   \item $E,a_0,..,v_4$ are constants, which are mapped to zero by all derivations
   \item $r$, $\Psi$, $\Psi'$, $\Psi''$, and $v$ are indeterminates that will ultimately be mapped into a function space,
   so often we can think of them as functions.  All we can really say about them at this point,
   aside from trivial statements like $dx(\Psi) = \Psi_x$
   is that they are not constants.
%%   \item An algebraic extension is used to relate $r$ to the radius
   \item Later we will specify a ranking.
\end{itemize}

% The Risch algorithm

% The use of differential Galois theory to analyze linear ODEs.

% The failure of differential Galois theory to answer the obvious question about PDE.

Combining the concepts of differential algebra and algebraic geometry together,
we obtain {\it differential algebraic geometry},
where we have systems of differential polynomials and seek find solutions when the
indeterminates are mapped to functions.

Mapping the indeterminates to constants, for example the complex numbers, isn't a fruitful
course of action since the derivatives of constants are always zero, so likewise will
our derivations map constants to zero.  Instead, we map the indeterminates to functions
in some function space.  Exactly which function space we use
depends on the specific problem.
Popular choices include complex analytic functions $C^\omega(\mathbb{C})$,
or a Sobolov space like $H^2(\mathbb{R}^n)$.
%Much as in the case of algebraic geometry, it is often most convenient
%to map indeterminates into the coefficient field, we can let the coefficient field
%be the function space.  This is by far the most common approach in differential algebraic geometry.

However, I find it useful to consider the function space as something quite
small, parametized by a finite number of constants, that we pick for our purpose of solving some particular
equation or system of equations.  The great advantage of this approach is that by using
a finite number of parameters to describe our function space, we can
use the tools of algebraic geometry to characterize the
solutions as algebraic varieties.

We do this by introducing
additional differential polynomials into our system.
This doesn't let us restrict our functions to
something like $L^2(\mathbb{R})$, because that requires imposing a global integral condition,
and there's no way to specify that using only differential polynomials.
We can, however, restrict the function space by requiring the solutions
to satisfy additional differential equations.

An algorithm begins to suggest itself.  To solve a given differential equation,
we pick a small function space
parameterized by a finite number of constants, describe it using a
system of differential polynomial equations, then use
some kind of a reduction algorithm on the original equation to extract a system of polynomial equations involving
only the constants.

What kinds of small function spaces are useful?
Starting with a PDE, I have obtained good results by restricting it to be solved using
ODEs of certain limited forms.  Parametizing those forms with a finite number
of constants yields a function space parametized by constants, and therefore
describing a solution variety in something like $\mathbb{C}^n$.  Such a
solution space is then ammenable to the techniques of algebraic geometry,
such as primary decomposition.
Each associated prime ideal in the resulting primary decomposition can be interpreted as a family
of solutions of the differential equation in the function space.

{\bf Example:} Starting with the following Schr\"odinger equation for hydrogen:

\begin{equation}
% \label{schrodinger hydrogen}
-\frac{1}{2}\nabla^2 \Psi - \frac{1}{r}\Psi = E \Psi
\end{equation}

We can require the solution to solve the following parametized ODE:

\begin{equation}
\begin{gathered}
(a_0 + a_1 v) \frac{d^2\Psi(v)}{dv^2} + (b_0 + b_1 v) \frac{d\Psi(v)}{dv} + (c_0 + c_1 v) \Psi(v) = 0
\end{gathered}
\end{equation}

where $\Psi(v)$ is a univariate function of some variable $v$, itself expressed in a parameterized form:

\begin{equation}
% \label{ansatz 5a}
v = v_1 x + v_2 y + v_3 z + v_4 r
\end{equation}

and expect to find that for certain constant values:

\begin{equation}
\begin{gathered}
a_1 = v_4 = 1 \qquad
b_0 = c_0 = 2 \qquad
c_1 = E \\
a_0 = b_1 = v_1 = v_2 = v_3 = 0
\end{gathered}
\end{equation}

we will obtain the classical radial equation for hydrogen:

\begin{equation}
% \label{classical eq in ideal}
\begin{gathered}
r \frac{d^2\Psi(r)}{dr^2} + 2 \frac{d\Psi(r)}{dr} + 2(1 + E r) \Psi(r) = 0
\end{gathered}
\end{equation}

a result more commonly derived using separation of variables.  Solving the radial equation
yields the ground state solution of hydrogen ($e^{-r}$ when $E=-1/2$).

%In the differential algebraic geometry setting, remember that the indeterminates
%take on values in some function field.  Otherwise, how could derivation be expressed?
%The best we're likely to reduce to is an equation like $\Psi=\Psi_x$, and it's up to us
%to solve the corresponding analytic equation and conclude that $\Psi$ is a multiple of the exponential of $x$.

%The picture, in my mind, starts to look like this:

%What kind of reduction step

%Practical experience with this algorithm suggests that, given the current
%state of software development, it is best to use a slightly inferior
%variant of this algorithm, where a standard Gr\"obner basis algorithm
%is used instead of Rosenfeld-Gr\"obner.  Considering that Rosenfeld-Gr\"obner
%is a generalization of Buchberger's algorithm, and given the computational
%complexity of computing a Gr\"obner basis, it makes sense to use
%existing Gr\"obner basis software, hoping that in the future, we
%will have a comprehensive Rosenfeld-Gr\"obner implementation that
%will handle differential polynomials as well as conventional polynomials,
%incorporate all the fruits of
%our research into the computation of Gr\"obner bases,
%and fall back into an efficient Gr\"obner basis calculation
%when presented with the degenerate case of a system of polynomial equations.

%We can describe a slightly inferior variant of this algorithm, that only
%requires the calculation of Gr\"obner bases to complete a primary decomposition.
%utility of algebraic geometry in the differential algebra setting by parameterizing
%the function space using only a finite number of constants, and doing so in such
%a way that the constants can be isolated into a system of polynomial equations,
%which can then be solving using algebraic geometry.

%What specific restrictions must be imposed to acheive this?

%  - new indeterminants must be specified as ODEs
%  - we need to cancel the leading derivative

%If, for example, we expect
%our solution to be in $C^2(\mathbf{R})$, then we parameterize a subset of
%$C^2(\mathbb{R})$ using some number of complex variables, say twenty, so that our resulting
%equations are polynomials in $\mathbb{C}[c_0,..,c_{19}]$ and their common
%solutions describe an algebraic variety in $\mathbb{C}^{20}$.
%Unfortunately, degree bounds seem necessary on almost everything in order
%to achieve this, and no matter how big and complicated our equations become,
%we're only searching part of the function space.

%An algorithm begins to suggest itself.  We can recover the
%utility of algebraic geometry in the differential algebra setting by parameterizing
%the function field using only a finite number of constants, and doing so in such
%a way that the constants can be isolated into a system of polynomial equations,
%which can then be solving using algebraic geometry.  If, for example, we expect
%our solution to be in $C^2(\mathbb{R})$, then we parameterize a subset of
%$C^2(\mathbb{R})$ using some number of complex variables, say twenty, so that our resulting
%equations are polynomials in $\mathbb{C}[c_0,..,c_{19}]$ and their common
%solutions describe an algebraic variety in $\mathbb{C}^{20}$.
%Unfortunately, degree bounds seem necessary on almost everything in order
%to achieve this, and no matter how big and complicated our equations become,
%we're only searching part of the function space.

In short, rather than attempt to derive differential equations that apply to all solutions
of the PDE, we restrict our attention to an {\bf ansatz}, a subset of the function
space in which we seek solutions, parameterized as a finite dimensional vector space over the constants.
If we can, by luck or by theory, pick a good ansatz, then we have
a computable algorithm to find the solutions therein.

\section{Solution Algorithm}
\subsection{The Ansatz}

These are the primary requirements
for an {\bf ansatz}:

\begin{itemize}
%% \item it must be formed from a finite sequence of linear ODE and algebraic extensions, and
\item it expresses a restricted form for the solution to the PDE
\item it is expressed using differential polynomials,
\item it may involve additional indeterminates, which may or may not be constants
\end{itemize}

%Additionally, I am specifically looking for solutions that can be constructed using ODEs.
One of the most fundamental questions to ask of a PDE is whether can it
be solved using ODEs.  We can't definitely answer this question using
this algorithm, but as these are the types of solutions I am interested in,
all of the current ansatzen attempt to express solutions to the PDE using
a finite number of ODEs.

For the present work, I restrict to homogeneous linear ODEs,
because the PDEs I am primarily interested in solving (various forms of Schr\"odinger's equation)
are homogeneous and linear, so I'm hopeful that homogeneous linear ODEs will suffice to solve them.
This is not a fundamental limitation of the algorithm.

A $n^{\rm th}$-order homogeneous linear ODE takes the form:

\begin{equation}
a_n(v) y^{(n)} + a_{n-1}(v) y^{(n-1)} \cdots a_1(v) y' + a_0(v) y = 0
\end{equation}

where $y^{(n)}, y^{(n-1)}, ... y'$ are derivatives of the dependent variable $y$ with respect to the independent variable $v$.

% How to introduce a new ODE?  First, we wish to avoid the complexities of introducing new derivations,
% so we seek to specify how the ODE behaves under the existing derivations.

We introduce the dependent variable $y$ by adjoining a new indeterminant that is defined using a differential polynomial
whose coefficients are all univariate functions of a single {\it independent variable} $v$,
and whose derivatives are all taken with respect to that
independent variable.  This independent variable $v$ is also a new indeterminant,
selected from the underlying field and itself parameterized by constants,
which is expressed using a simple equality between the new indeterminant $v$ and its parameterized form.

The coefficients of the ODE's defining differential polynomial are
selected not from the existing field, but rather from ${\mathbb Q}(v)$, an auxiliary field
constructed using the independent variable $v$.
We could also consider coefficients selected from an extension
of ${\mathbb Q}(v)$, but that is not required for the present work.

{\bf Example:} Over the rational function field $\mathbb{C}(x)$, we can construct an ODE
extension specified by the dependent indeterminate $\Psi$, independent indeterminate $v$, and differential polynomial
$\Psi' = \Psi$.  This differential polynomial
maps to the analytic equation $\frac{d\Psi}{dv}=\Psi$ and is solved
by the analytic functions $Ae^v$ ($A$ is an arbitrary constant).  If $v=x+r$, then
the functions $Ae^{x+r}$ are solutions to this ODE extension.

How can derivatives with respect to a new independent variable be connected with the existing derivations?
We can expess our previously existing derivatives in terms of the new one using the chain rule.
In fact, all that required is to specify how the new interderminates behave under the existing
derivations, and that relationship is simply ${\rm d}\Psi/{\rm d}x = {\rm d}\Psi/{\rm d}v\cdot {\rm d}v/{\rm d}x$ (analytically).
Algebraically, we'll introduce a new indeterminate $\Psi'$ and write $\Psi_x = \Psi' v_x$

I use apostrophes, and not the jet notation subscripts, when writing
the differential polynomial in the ODE case to emphasize that we are not introducing a new derivation,
nor is the ODE's derivative any of the existing derivatives.

In short, to introduce a new ODE (second-order, for the purposes of illustration):

\begin{itemize}
% \item We wish to avoid introducing new derivations.
\item We introduce new indeterminates $\Psi$, $\Psi'$, $\Psi'', v$
\item We introduce a new differential polynomial involving only $\Psi''$, $\Psi'$, $\Psi$, $v$, and constants,
and using no non-constant indeterminates or derivatives
\item We define the derivatives of $\Psi$ and $\Psi'$ using the chain rule: \break $\Psi_x = \Psi' v_x$; $\Psi'_x = \Psi'' v_x$
\item We equate the independent variable $v$ to some parameterized differential polynomial
\end{itemize}

All derivatives of $\Psi''$ are now completely defined by these differential polynomials.
%(assuming that polynomial is first degree in $\Psi''$, and its initial involves only $v$)
%(the derivations would be defined even without this restriction, but might involve $\Psi''$ itself)
%(the assumption allows $\Psi''$ to eliminated by differential reduction)

\tikzstyle{poly}=[rectangle, draw, thick, fill=white, text width=5em, align=center, rounded corners, minimum height=2em]
\tikzstyle{poly2}=[rectangle, draw, thick, fill=white, align=center, rounded corners, minimum height=2em]
\tikzstyle{ring}=[rectangle, draw=blue, thick, fill=blue!20, text width=5em,align=center, rounded corners, minimum height=2em]
\tikzstyle{element}=[rectangle, draw=orange, thick, fill=orange!20, align=center, rounded corners, minimum height=2em]
\tikzstyle{algebraic}=[rectangle, draw=green, thick, fill=green!20, align=center, rounded corners, minimum height=2em]
\tikzstyle{degree}=[]

%% {\bf Ansatz 5}

\begin{figure}
\centering
  \begin{tikzpicture}[remember picture,out=315,in=225,distance=0.4cm,node distance=40pt]
    \node (Psi) [poly, text width=100pt] {$\framebox(10,10){}\tikzmark{a}\,\Psi'' + \framebox(10,10){}\tikzmark{b}\,\Psi' + \coeff\tikzmark{c}\,\Psi$};
    \node (Qv) [ring, below of=Psi, text width=100pt] {$\mathbb{Q}[v]$};
    \node (v) [poly, right=of Qv] {$v$};
    \draw[degree] (a.west) -- (a.west|-Qv.north) node[right,pos=0.6] {1};
    \draw[degree] (b.west) -- (b.west|-Qv.north) node[right,pos=0.6] {1};
    \draw[degree] (c.west) -- (c.west|-Qv.north) node[right,pos=0.6] {1};
    \node (Psi label) [at=(v.east|-Psi.north), anchor=north east] {\Large$\Psi$};
    \begin{scope}[on background layer]
        \node (Psi block)[fit=(Psi) (Qv) (v), inner sep=10pt, element] {};
    \end{scope}
    \node (base) [ring, node distance=70pt, below=of Psi block.east, anchor=east, text width=250pt] {$\mathbb{Q}[x,y,z,r]/(r^2-x^2-y^2-z^2)$};
    \draw[degree] (v.south) -- (v.south|-base.north) node[right,pos=0.7] {1};
  \end{tikzpicture}
\caption{An ansatz for a second-order homogenous ODE}
\label{ansatz 5 figure}
\end{figure}

For example, consider the ansatz formed from 
a second-order ODE element with first-degree coefficients and an independent variable formed
as a first-degree polynomial in the underlying differential polynomial ring.
See figure \ref{ansatz 5 figure}, where polynomial degrees are signified by the black numbers next to the lines connecting them to the rings they are selected from.
%Note in particular that there is no green box around the orange one.  The ODE
%element must itself solve the PDE; we do not construct the full extension field it would naturally generate.
We do not construct any more complicated function from $\Psi$ (although we could);
the element $\Psi$ must itself solve the PDE.

The structure of this ansatz is described using differential polynomials as follows.

A differential polynomial involving only $\Psi''$, $\Psi'$, $\Psi$, $v$, and constants,
and using no non-constant indeterminates or derivatives.

\begin{equation}
\label{ansatz 5b}
(a_0 + a_1 v) \Psi'' + (b_0 + b_1 v) \Psi' + (c_0 + c_1 v) \Psi = 0 \\
\end{equation}

The derivatives of $\Psi$ and $\Psi'$, defined using the chain rule:

\begin{equation}
\label{ansatz 5c}
\begin{gathered}
\Psi_x = \Psi' v_x \qquad
\Psi_y = \Psi' v_y \qquad
\Psi_z = \Psi' v_z \\
\Psi'_x = \Psi'' v_x \qquad
\Psi'_y = \Psi'' v_y \qquad
\Psi'_z = \Psi'' v_z \\
\end{gathered}
\end{equation}

The independent variable $v$:

\begin{equation}
\label{ansatz 5a}
v = v_1 x + v_2 y + v_3 z + v_4 r
\end{equation}

Our differential ring is equipped with three derivations $dx$, $dy$, and $dz$
(this is not apparent in the graphical representation).  Our base ring
is $\mathbb{Q}[x,y,z,r]/(r^2-x^2-y^2-z^2)$, and from this we select
a linear variable $v$ using equation \eqref{ansatz 5a}.
The $v_i$'s (and the $a_i$'s, $b_i$'s and $c_i$'s) are constants.
Strictly speaking, a $v_0$ constant term
should also be present, but I often drop the constant term from the
independent variable of an ODE extension, because taking
the derivative with respect to $x+3$ is no different from taking
the derivative with respect to $x$.

Next we introduce our new ODE element $\Psi$, which will
also be the solution to our PDE.
We now wish to essentially introduce a new derivative with respect to $v$.
While I could write $\Psi_v$, I do not wish to confuse this derivative
with the existing three derivations, plus I wish to emphasis
that we don't need to actually introduce any new derivations,
so I'll write this as $\Psi'$.  Treat $\Psi$, $\Psi'$, and
$\Psi''$ as new indeterminates.
$\Psi$ must satisfy the minimial differential polynomial, which
we specify in equation \eqref{ansatz 5b}, also introducing
degree bounds on the coefficients.

Finally, we need to connect our new derivatives with the existing derivations.
Equations \eqref{ansatz 5c} show how to evaluate
the derivatives of $\Psi$ and $\Psi'$ with respect to the
existing derivations in the differential ring (it's just the chain rule).
Although conceptually we introduce another derivative, with respect to the ODE's independent variable, it is important
to note that we do not have to add any new derivations to the original differential ring.
All that is needed is
to add $\Psi$, $\Psi'$, and $\Psi''$ as new indeterminates
and specify how they behave with respect to the existing derivations.
The behavior of $\Psi$ and $\Psi'$ is specified with the chain rule;
once their behavior has been defined, the behavior of $\Psi''$ is implied by the ODE.

\subsection{Differential Reduction}

Having defined an ansatz, we now use it to reduce our original PDE.  For this purpose,
we use Ritt's full reduction algorithm; see \cite{Ritt} Chapter I, \S 6 or \cite{Kolchin}, Chapter I, \S 9.
A compact description of the relevant concepts can be found in Section 3 of \cite{blop}.
%% A comparison of various techniques used to solve systems of polynomial and differential polynomial equations can be found in \cite{aistleitner}.

Ritt's reduction algorithm operates on differential polynomials contained in a differential ring,
and requires a {\it ranking} to be specified on the indeterminants and their derivatives.
This is a distinct concept from an ordering on the monomials, as used, for example, in defining a Gr\"obner basis.

A ranking on a differential ring $R$ is an ordering on the ring's indeterminates and their derivatives that meets two compatibility conditions:

\begin{equation}
\begin{gathered}
\label{ranking compatibility}
u < v \implies \delta u < \delta v \quad \forall \delta\in\Theta, \, \forall u,v \in R\\
u < \delta u \quad \forall \delta\in\Theta, \, \forall u \in R
\end{gathered}
\end{equation}

The {\it order} of a differential operator ${\rm ord}\ \delta$ is the sum of all its powers.  A ranking is said to be {\it orderly}
if meets the additional condition:

\begin{equation}
{\rm ord}\ \delta < {\rm ord}\ \theta \implies \delta u < \theta v \quad \forall \delta,\theta,u,v
\end{equation}

The {\it leader} of a polynomial is the highest ranking indeterminant or derivative that appears in the polynomial
with a non-zero coefficient.

A differential polynomial $p$ is said to be {\it partially reduced} with respect to a differential polynomial $q$ if
no proper derivative of $q$'s leader appears in $p$, and {\it fully reduced} if it is partially reduced and
$q$'s leader, if it appears in $p$, appears at a lower degree than its degree in $q$.

% To reduce a differential polynomial $p$ by a system $S$ of differential polynomials,
% we see if there exists a polynomial in $S$ any higher derivative of the leader of a polynomial in $S$ appears in $p$.
% If so, we pick the highest ranking

Ritt's full reduction algorithm reduces a differential polynomial $p$ by a system $S$ of differential polynomials
and produces differential polynomials $r$ (the remainder) and $h$ (the ``denominator'')
such that $r$ is fully reduced with respect to all of the differential polynomials in $S$
and $(hp - r)$ is an element of the differential ideal $[S]$ generated by the elements of $S$ and all their derivatives.

\subsection{Projection}

Having defined an ansatz and used it to reduce the original PDE, we now project
that reduced equation into the solution space of the constants.  We do this
by factoring each term in the reduced PDE into its constant and non-constant indeterminants,
treating derivatives as separate indeterminates,
and gathering together like terms with identical non-constant factors.  Setting the
constant coefficients of each like term to zero produces a system of polynomial equations,
involving only the constants, that will solve the original PDE when satisfied.

\begin{comment}
Taking the radical of the ideal simplifies both the theory and the calculation,
and can be justified because there are no nilpotent elements in our solution
space, which is just $n$-dimensional complex space.  The
primary decomposition of a radical ideal is also a prime decomposition,
as the distinction between primary and prime ideals is only significant
for non-radical ideals.
\end{comment}

This system of equations defines an algebraic variety in $\mathbb{C}^n$, but it need not be irreducible,
meaning that it can be expressed as the union of simpler, irreducible varieties, and it generally
advantageous to compute such a decomposition.
There is a well-known correspondence between prime ideals and irreducible varieties
(\cite{iva} Corollary 4.5.4)), and
several algorithms have been proposed and implemented to compute the minimal associated prime ideals
that describe such an irreducible decomposition (\cite{GTZ}, among others).

\begin{comment}
Any particular selection of constants produces a system of differential polynomials that defines a differential ideal.

Which of those differential ideals reduce the original PDE to zero?

i.e, I^i S^s PDE = [A]

(A, I, and S are parameterized by constants)

if all of the A's are equal to zero, then either the PDE is equal to zero or one of the I's and/or S's is equal to zero

for some selection of the constants, this creates a specific A, I, and S.  Q: does this solve the PDE?
   what does this question mean?
   does setting all of the A's to zero (i.e, make them all true) imply that the PDE is zero?
   it implies that either the PDE is zero, or an I/S is zero

in my case, I'm ``ignoring the denominator'', meaning that I'm ignoring I and S.  If I or S were to be zero,
then the PDE would not have to be zero, i.e, would not have to be satisfied

So, we want to check that the ``denominator'' (I and S) is not zero.  If they were, the PDE might not be satisfied.

If an I or an S is zero, then we could add it to the system and check again

For any solution in the variety of the constants, if I/S is zero, then it might not solve the PDE.
Q: what do we do then?

Q: if some selection of constants solves the PDE, do we always find it using this algorithm?
\end{comment}

\subsection{Solution Algorithm}

To summarize, given a PDE defined by one or more differential polynomials, do the following:

\begin{enumerate}
\item Select an ansatz that defines a differential function space parameterized by constants

\item Reduce the PDE modulo the ansatz (differential elimination step)

\item Construct a system of polynomial equations in the constants by collecting like terms in the remaining variables (projection step)

\item Form a polynomial ideal from that system of equations, and compute its minimal associated prime ideals

\item Each prime ideal corresponds to an irreducible variety in the space of constants,
the union of which form the solution space of the PDE in the differential function space

\end{enumerate}

%%\section*{An Example: The New Solution of Hydrogen}
%%\section{Example: A New Pseudo-Solution of Hydrogen}
\section{Example: A New Solution of Hydrogen}

We seek to demonstrate the operation of the algorithm on the following Schr\"odinger equation for hydrogen:

\begin{equation}
\label{schrodinger hydrogen}
-\frac{1}{2}\nabla^2 \Psi - \frac{1}{r}\Psi = E \Psi
\end{equation}

where $\nabla$ is the Laplacian, $\Psi$ is a complex-valued wavefunction defined over real 3-dimensional space,
and $r$ is the distance to the origin.  We choose to work in Cartesian coordinates, so $r^2=x^2+y^2+z^2$,
and use Hartree atomic units to render the equation dimensionless, though for our purposes we can
ignore any physical interpretation of the problem and treat it simply as an equation to be solved.

We'll use the ansatz from the previous section, a second order ODE with first degree coefficients and a first degree independent variable, i.e:

\begin{center}
  \begin{tikzpicture}[remember picture,out=315,in=225,distance=0.4cm,node distance=40pt]
    \node (Psi) [poly, text width=100pt] {$\framebox(10,10){}\tikzmark{a}\,\Psi'' + \framebox(10,10){}\tikzmark{b}\,\Psi' + \coeff\tikzmark{c}\,\Psi$};
    \node (Qv) [ring, below of=Psi, text width=100pt] {$\mathbb{Q}[v]$};
    \node (v) [poly, right=of Qv] {$v$};
    \draw[degree] (a.west) -- (a.west|-Qv.north) node[right,pos=0.6] {1};
    \draw[degree] (b.west) -- (b.west|-Qv.north) node[right,pos=0.6] {1};
    \draw[degree] (c.west) -- (c.west|-Qv.north) node[right,pos=0.6] {1};
    \node (Psi label) [at=(v.east|-Psi.north), anchor=north east] {\Large$\Psi$};
    \begin{scope}[on background layer]
        \node (Psi block)[fit=(Psi) (Qv) (v), inner sep=10pt, element] {};
    \end{scope}
    \node (base) [ring, node distance=70pt, below=of Psi block.east, anchor=east, text width=250pt] {$\mathbb{Q}[x,y,z,r]/(r^2-x^2-y^2-z^2)$};
    \draw[degree] (v.south) -- (v.south|-base.north) node[right,pos=0.7] {1};
  \end{tikzpicture}
\end{center}

\begin{equation}
\label{ansatz 5}
\begin{gathered}
\begin{comment}
\Psi_x = \frac{\Psi'}{v_x} \qquad
\Psi_y = \frac{\Psi'}{v_y} \qquad
\Psi_z = \frac{\Psi'}{v_z} \\
\Psi'_x = \frac{\Psi''}{v_x} \qquad
\Psi'_y = \frac{\Psi''}{v_y} \qquad
\Psi'_z = \frac{\Psi''}{v_z} \\
\end{comment}
\Psi_x = \Psi' v_x \qquad
\Psi_y = \Psi' v_y \qquad
\Psi_z = \Psi' v_z \\
\Psi'_x = \Psi'' v_x \qquad
\Psi'_y = \Psi'' v_y \qquad
\Psi'_z = \Psi'' v_z \\
(a_0 + a_1 v) \Psi'' + (b_0 + b_1 v) \Psi' + (c_0 + c_1 v) \Psi = 0 \\
v = v_1 x + v_2 y + v_3 z + v_4 r
\end{gathered}
\end{equation}

\subsection{Differential Reduction Step}

%I attempted to use the Rosenfeld-Gr\"obner algorithm to reduce equation \eqref{schrodinger hydrogen}
%modulo \eqref{ansatz 5}, but it ultimately ran out of memory on a 96 GB computer after
%30 hours.  Instead, I used Sage to construct a polynomial ring modulo the ideal
%$r^2-x^2-y^2-z^2$ to handle the algebraic extension (present not in the ansatz proper,
%but in the original ring used to construct the PDE).  Next I directed the software
%to expand the derivatives in \eqref{schrodinger hydrogen} and substitute for $\Psi''$ and $v$
%according to the ansatz \eqref{ansatz 5}.  The result is
%a rational function
%with a 228 term numerator and an 18 term denominator.  We ignore the denominator.  The numerator begins:

To \eqref{ansatz 5} we append
$r^2-x^2-y^2-z^2$ to handle the algebraic extension used
in the underlying polynomial ring to construct the PDE, and obtain
the following system of differential polynomials:

\begin{subequations}
\label{ansatz 5 plus}
\begin{gather}
\Psi_x - \Psi' v_x \qquad
\Psi_y - \Psi' v_y \qquad
\Psi_z - \Psi' v_z \\
\Psi'_x - \Psi'' v_x \qquad
\Psi'_y - \Psi'' v_y \qquad
\Psi'_z - \Psi'' v_z \\
(a_0 + a_1 v) \Psi'' + (b_0 + b_1 v) \Psi' + (c_0 + c_1 v) \Psi \label{ansatz 5 plus:1} \\
v - (v_1 x + v_2 y + v_3 z + v_4 r) \label{ansatz 5 plus:2} \\
r^2 - (x^2 + y^2 + z^2)
\end{gather}
\end{subequations}

\begin{comment}
\eqref{ansatz 5 plus} (ordering? also, they're equations, not polynomials) is a differentially triangular system, although this property
is not required to ensure the correct operation of the algorithm.  Next, I used Sage,
along with some additional Python code, to differentially reduce \eqref{schrodinger hydrogen}
modulo \eqref{ansatz 5 plus}, using Ritt's reduction algorithm.
The result is a rational function
with a 228 term numerator and an 18 term denominator.  The numerator begins:
\end{comment}

% I used a Sage-based computer program with the following ansatz.
\begin{comment}
I found this solution roughly as follows.\footnote{
I discovered an alternate form of this solution using a somewhat more complex ansatz
on January 24, 2023.  By January 26, I had established the solution in its current form.
The original ansatz produced a rational function with
a 1254 term numerator and a 36 term denominator, that gave rise to a system of 224 equations,
and was first solved using numerical approximation (scipy.optimize.root).
}

Use Cartesian coordinates.  Let $v$ be a linear polynomial in the coordinates and the root $r=\sqrt{x^2+y^2+z^2}$,
Reduce the input PDE \eqref{schrodinger} modulo differential ideal \eqref{ansatz 5}, obtaining
\end{comment}

What should our ranking be?

We want $\Psi''$ to be eliminated in favor of $\Psi'$ and $\Psi$, so we need $\Psi''$
to be the leader of \eqref{ansatz 5 plus:1}.  This requires that $\Psi''$ be ranked above $\Psi'$,
$\Psi$, $v$, and the constants, suggesting

\begin{equation}
\label{ranking 1}
\Psi'' > \Psi' > \Psi > v > {\rm constants}
\end{equation}

Also, we want the
derivatives of $\Psi$ and $\Psi'$ (like $\Psi_x$) to be eliminated in favor of $\Psi'$ and $\Psi''$,
so we need $\Psi_x$ to be the leader of $\Psi_x - \Psi' v_x$.
This requires that $\Psi_x$ be ranked above both $\Psi'$ and $v_x$.
\eqref{ranking 1} and \eqref{ranking compatibility} already imply that $\Psi_x > v_x$.
$\Psi_x$ will be ranked higher than $\Psi'$ if \eqref{ranking 1} is ranked
in an {\it orderly} manner,
meaning that all of the derivatives (like $\Psi_x$) rank higher than the pure indeterminates (like $\Psi'$).

We want the derivatives of $v$ to be eliminated in favor of $r$, so we need $v > r$.
For example, applying $dx$ to \eqref{ansatz 5 plus:2}, we obtain

\begin{equation}
\begin{gathered}
\label{vx derivation}
%%v - (v_1 x + v_2 y + v_3 z + v_4 r) \Longrightarrow v_x - v_4 r_x \\
%%r^2 - (x^2 + y^2 + z^2) \Longrightarrow 2 r r_x - 2 x
v - (v_1 x + v_2 y + v_3 z + v_4 r) \xRightarrow{\quad dx\quad} v_x - v_4 r_x \\
% r^2 - (x^2 + y^2 + z^2) \xRightarrow{\quad dx\quad} 2 r r_x - 2 x
\end{gathered}
\end{equation}

$v_x$ will be the leader of \eqref{vx derivation} if $v$ is ranked greater than both $r$ and the constants, suggesting:

\begin{equation}
\Psi'' > \Psi' > \Psi > v > r > {\rm constants}
\end{equation}

Finally, the {\tt DifferentialAlgebra} library \cite{DifferentialAlgebra} always ranks
indeterminates associated with derivations below everything else, so we use this orderly ranking:

\begin{equation}
\Psi'' > \Psi' > \Psi > v > r > {\rm constants} > x,y,z
\end{equation}

Applying Ritt's full reduction algorithm to \eqref{schrodinger hydrogen} modulo \eqref{ansatz 5 plus}, we obtain
the remainder:

\begin{equation}
32 ( x^{2} + y^{2} + z^{2} ) p
\end{equation}

where $p$ is an 85 term differential polynomial that begins:

\begin{equation}
\label{schrodinger modulo ansatz 5}
%sage: print(latex(sum(eq_a_reduceRing_n.terms()[0:4])).replace('Zeta', 'Psi'))
% then add "+ \cdots" at the end
%r \Psi' x^{3} v_{1}^{3} b_{1} + r \Psi' x y^{2} v_{1}^{3} b_{1} + r \Psi' x z^{2} v_{1}^{3} b_{1} + r \Psi' x^{2} y v_{1}^{2} v_{2} b_{1} + \cdots
%-2*E*a0*Psi*r - 2*E*a1*v1*x*Psi*r - 2*E*a1*v2*y*Psi*r - 2*E*a1*v3*z*Psi*r
x \Psi' r v_{1}^{3} b_{1} + y \Psi' r v_{1}^{2} v_{2} b_{1} + x \Psi' r v_{1} v_{2}^{2} b_{1} + y \Psi' r v_{2}^{3} b_{1} + \cdots
\end{equation}

% The denominator, after replacing $x^2+y^2+z^2$ with $r^2$, reads:

$h$ (the ``denominator'') reads:

\begin{equation}
% 2 r^3 (a_1 (v_1 x + v_2 y + v_3 z + v_4 r) + a_0)
%64 v r^{2} a_{1} + 64 r^{2} a_{0}
64 r^{2} (v a_{1} + a_{0})
\end{equation}

\begin{comment}
What does this mean?

If I took the original differential polynomial (PDE), multiplied it by $h$, and subtracted the remainder, I would get something
in the differential ideal.

In particular, if I multiplied by $r^2$ and subtracted the remainder, which factors, I'd get something like this:

        r^2 PDE - (x^2 + y^2 + z^2) p   \in  [S]

I've already expanded PDE into a rational function and discarded its denominator.  $r$ appears in PDE to the first degree.

$p$ includes $r$, to the first degree, but can't include $r$ to a higher power because then $p$ wouldn't be reduced.

[S] includes $r^2 - (x^2 + y^2 + z^2)$, and therefore $r^2 PDE - (x^2 + y^2 + z^2) PDE$
\end{comment}

\subsection{Projection Step}

Having reduced our PDE \eqref{schrodinger hydrogen} by the differential ideal defined by \eqref{ansatz 5},
we now wish to project our solution into the vector space of the constants.  We're looking for
constants that will solve equation \eqref{schrodinger modulo ansatz 5} for all values of $x$, $y$, $z$, $r$,
$\Psi$, and $\Psi'$, so
we collect like terms in $x$, $y$, $z$, $r$, $\Psi$, and $\Psi'$, organizing the numerator like this:

\begin{equation}
%sage: print(latex(Factorization([(e,1) for e in list(system_of_like_terms.items())[0]], sort=False)).replace('Zeta','Psi'))
%(r \Psi' x^{3}) \cdot (v_{1}^{3} b_{1} + v_{1} v_{2}^{2} b_{1} + v_{1} v_{3}^{2} b_{1} + 3 v_{1} v_{4}^{2} b_{1})
% then drop the parens and the \cdot and add plus sign and \cdots at the end
r \Psi' x^{3} (v_{1}^{3} b_{1} + v_{1} v_{2}^{2} b_{1} + v_{1} v_{3}^{2} b_{1} + 3 v_{1} v_{4}^{2} b_{1}) + \cdots
\end{equation}

The expressions in parenthesis (only one is shown) gives us a system of equations
involving only constants
that, if satisfied,
will yield a solution to \eqref{schrodinger hydrogen} in the form \eqref{ansatz 5}.
Note that $E$, although
not a parameter in the ansatz, is included with them because it is a constant.  The
correct operation of the algorithm is predicated on nothing more than separating
the indeterminants into constants and non-constants, and it makes no difference whether
the constants are introduced in the ansatz or appear in the original PDE.
The system has 28 equations:

%% The problem with making this a "figure" is that it's a numbered system of equations
\begin{equation}
\label{polynomial system}
% sage: load('joca.sage')
% sage: bwb = build_system_of_equations(PolyRing(r.args[2]), PolyRing_constants)
% sage: latex(matrix(sorted(bwb)).transpose())
% then copy-and-paste
\begin{array}{r}
-2 v_{1} v_{4} a_{1} + 2 v_{1} v_{4} b_{0} \\
-2 v_{2} v_{4} a_{1} + 2 v_{2} v_{4} b_{0} \\
-2 v_{3} v_{4} a_{1} + 2 v_{3} v_{4} b_{0} \\
-2 v_{4}^{2} a_{1} + v_{1}^{2} b_{0} + v_{2}^{2} b_{0} + v_{3}^{2} b_{0} + v_{4}^{2} b_{0} \\
-2 v_{4} a_{0} \\
-2 a_{0} \\
2 v_{3} v_{4} c_{0} - 2 v_{3} a_{1} \\
2 v_{2} v_{4} c_{0} - 2 v_{2} a_{1} \\
2 v_{1} v_{4} c_{0} - 2 v_{1} a_{1} \\
v_{1}^{2} c_{0} + v_{2}^{2} c_{0} + v_{3}^{2} c_{0} + v_{4}^{2} c_{0} - 2 E a_{0} - 2 v_{4} a_{1} \\
4 v_{2} v_{3} v_{4} c_{1} \\
4 v_{1} v_{3} v_{4} c_{1} \\
4 v_{1} v_{2} v_{4} c_{1} \\
v_{1}^{2} v_{4} c_{1} + v_{2}^{2} v_{4} c_{1} + 3 v_{3}^{2} v_{4} c_{1} + v_{4}^{3} c_{1} - 2 E v_{4} a_{1} \\
v_{1}^{2} v_{4} c_{1} + 3 v_{2}^{2} v_{4} c_{1} + v_{3}^{2} v_{4} c_{1} + v_{4}^{3} c_{1} - 2 E v_{4} a_{1} \\
3 v_{1}^{2} v_{4} c_{1} + v_{2}^{2} v_{4} c_{1} + v_{3}^{2} v_{4} c_{1} + v_{4}^{3} c_{1} - 2 E v_{4} a_{1} \\
v_{1}^{2} v_{3} c_{1} + v_{2}^{2} v_{3} c_{1} + v_{3}^{3} c_{1} + 3 v_{3} v_{4}^{2} c_{1} - 2 E v_{3} a_{1} \\
v_{1}^{2} v_{2} c_{1} + v_{2}^{3} c_{1} + v_{2} v_{3}^{2} c_{1} + 3 v_{2} v_{4}^{2} c_{1} - 2 E v_{2} a_{1} \\
v_{1}^{3} c_{1} + v_{1} v_{2}^{2} c_{1} + v_{1} v_{3}^{2} c_{1} + 3 v_{1} v_{4}^{2} c_{1} - 2 E v_{1} a_{1} \\
4 v_{2} v_{3} v_{4} b_{1} \\
4 v_{1} v_{3} v_{4} b_{1} \\
4 v_{1} v_{2} v_{4} b_{1} \\
v_{1}^{2} v_{4} b_{1} + v_{2}^{2} v_{4} b_{1} + 3 v_{3}^{2} v_{4} b_{1} + v_{4}^{3} b_{1} \\
v_{1}^{2} v_{4} b_{1} + 3 v_{2}^{2} v_{4} b_{1} + v_{3}^{2} v_{4} b_{1} + v_{4}^{3} b_{1} \\
3 v_{1}^{2} v_{4} b_{1} + v_{2}^{2} v_{4} b_{1} + v_{3}^{2} v_{4} b_{1} + v_{4}^{3} b_{1} \\
v_{1}^{2} v_{3} b_{1} + v_{2}^{2} v_{3} b_{1} + v_{3}^{3} b_{1} + 3 v_{3} v_{4}^{2} b_{1} \\
v_{1}^{2} v_{2} b_{1} + v_{2}^{3} b_{1} + v_{2} v_{3}^{2} b_{1} + 3 v_{2} v_{4}^{2} b_{1} \\
v_{1}^{3} b_{1} + v_{1} v_{2}^{2} b_{1} + v_{1} v_{3}^{2} b_{1} + 3 v_{1} v_{4}^{2} b_{1}
\end{array}
\end{equation}

% This is the unsorted list of equations
\begin{comment}
\begin{equation}
\label{polynomial system}
% sage: load('joca.sage')
% sage: bwb = build_system_of_equations(PolyRing(r.args[2]), PolyRing_constants)
% sage: latex(matrix(bwb).transpose())
% then copy-and-paste
\begin{array}{r}
4 v_{1} v_{2} v_{4} c_{1} \\
4 v_{1} v_{3} v_{4} c_{1} \\
-2 a_{0} \\
v_{1}^{2} v_{4} c_{1} + v_{2}^{2} v_{4} c_{1} + 3 v_{3}^{2} v_{4} c_{1} + v_{4}^{3} c_{1} - 2 E v_{4} a_{1} \\
v_{1}^{2} v_{4} b_{1} + 3 v_{2}^{2} v_{4} b_{1} + v_{3}^{2} v_{4} b_{1} + v_{4}^{3} b_{1} \\
v_{1}^{2} v_{2} c_{1} + v_{2}^{3} c_{1} + v_{2} v_{3}^{2} c_{1} + 3 v_{2} v_{4}^{2} c_{1} - 2 E v_{2} a_{1} \\
v_{1}^{2} v_{4} c_{1} + 3 v_{2}^{2} v_{4} c_{1} + v_{3}^{2} v_{4} c_{1} + v_{4}^{3} c_{1} - 2 E v_{4} a_{1} \\
4 v_{1} v_{3} v_{4} b_{1} \\
-2 v_{1} v_{4} a_{1} + 2 v_{1} v_{4} b_{0} \\
4 v_{1} v_{2} v_{4} b_{1} \\
-2 v_{3} v_{4} a_{1} + 2 v_{3} v_{4} b_{0} \\
v_{1}^{2} c_{0} + v_{2}^{2} c_{0} + v_{3}^{2} c_{0} + v_{4}^{2} c_{0} - 2 E a_{0} - 2 v_{4} a_{1} \\
2 v_{1} v_{4} c_{0} - 2 v_{1} a_{1} \\
-2 v_{2} v_{4} a_{1} + 2 v_{2} v_{4} b_{0} \\
v_{1}^{2} v_{2} b_{1} + v_{2}^{3} b_{1} + v_{2} v_{3}^{2} b_{1} + 3 v_{2} v_{4}^{2} b_{1} \\
4 v_{2} v_{3} v_{4} b_{1} \\
v_{1}^{3} b_{1} + v_{1} v_{2}^{2} b_{1} + v_{1} v_{3}^{2} b_{1} + 3 v_{1} v_{4}^{2} b_{1} \\
v_{1}^{2} v_{3} b_{1} + v_{2}^{2} v_{3} b_{1} + v_{3}^{3} b_{1} + 3 v_{3} v_{4}^{2} b_{1} \\
-2 v_{4}^{2} a_{1} + v_{1}^{2} b_{0} + v_{2}^{2} b_{0} + v_{3}^{2} b_{0} + v_{4}^{2} b_{0} \\
3 v_{1}^{2} v_{4} b_{1} + v_{2}^{2} v_{4} b_{1} + v_{3}^{2} v_{4} b_{1} + v_{4}^{3} b_{1} \\
-2 v_{4} a_{0} \\
v_{1}^{2} v_{4} b_{1} + v_{2}^{2} v_{4} b_{1} + 3 v_{3}^{2} v_{4} b_{1} + v_{4}^{3} b_{1} \\
v_{1}^{2} v_{3} c_{1} + v_{2}^{2} v_{3} c_{1} + v_{3}^{3} c_{1} + 3 v_{3} v_{4}^{2} c_{1} - 2 E v_{3} a_{1} \\
3 v_{1}^{2} v_{4} c_{1} + v_{2}^{2} v_{4} c_{1} + v_{3}^{2} v_{4} c_{1} + v_{4}^{3} c_{1} - 2 E v_{4} a_{1} \\
2 v_{2} v_{4} c_{0} - 2 v_{2} a_{1} \\
4 v_{2} v_{3} v_{4} c_{1} \\
v_{1}^{3} c_{1} + v_{1} v_{2}^{2} c_{1} + v_{1} v_{3}^{2} c_{1} + 3 v_{1} v_{4}^{2} c_{1} - 2 E v_{1} a_{1} \\
2 v_{3} v_{4} c_{0} - 2 v_{3} a_{1}
\end{array}
\end{equation}
\end{comment}

% This is the system of 34 equations that results if we don't remove the 32(x^2 + y^2 + z^2) factor
\begin{comment}
\begin{equation}
\label{polynomial system}
% sage: load('joca.sage')
% sage: latex(matrix(eqns).transpose())
% then copy-and-paste
\begin{array}{r}
-64 v_{4}^{2} a_{1} + 32 v_{1}^{2} b_{0} + 32 v_{2}^{2} b_{0} + 32 v_{3}^{2} b_{0} + 32 v_{4}^{2} b_{0} \\
32 v_{1}^{2} v_{3} c_{1} + 32 v_{2}^{2} v_{3} c_{1} + 32 v_{3}^{3} c_{1} + 96 v_{3} v_{4}^{2} c_{1} - 64 E v_{3} a_{1} \\
64 v_{1} v_{4} c_{0} - 64 v_{1} a_{1} \\
-64 v_{1} v_{4} a_{1} + 64 v_{1} v_{4} b_{0} \\
128 v_{1} v_{2} v_{4} c_{1} \\
128 v_{1} v_{2} v_{4} b_{1} \\
32 v_{1}^{2} c_{0} + 32 v_{2}^{2} c_{0} + 32 v_{3}^{2} c_{0} + 32 v_{4}^{2} c_{0} - 64 E a_{0} - 64 v_{4} a_{1} \\
64 v_{3} v_{4} c_{0} - 64 v_{3} a_{1} \\
-64 a_{0} \\
32 v_{1}^{3} c_{1} + 32 v_{1} v_{2}^{2} c_{1} + 32 v_{1} v_{3}^{2} c_{1} + 96 v_{1} v_{4}^{2} c_{1} - 64 E v_{1} a_{1} \\
96 v_{1}^{2} v_{4} c_{1} + 32 v_{2}^{2} v_{4} c_{1} + 32 v_{3}^{2} v_{4} c_{1} + 32 v_{4}^{3} c_{1} - 64 E v_{4} a_{1} \\
32 v_{1}^{2} v_{4} c_{1} + 96 v_{2}^{2} v_{4} c_{1} + 32 v_{3}^{2} v_{4} c_{1} + 32 v_{4}^{3} c_{1} - 64 E v_{4} a_{1} \\
32 v_{1}^{2} v_{4} c_{1} + 32 v_{2}^{2} v_{4} c_{1} + 96 v_{3}^{2} v_{4} c_{1} + 32 v_{4}^{3} c_{1} - 64 E v_{4} a_{1} \\
-64 v_{4} a_{0} \\
128 v_{1} v_{3} v_{4} c_{1} \\
96 v_{1}^{2} v_{4} b_{1} + 32 v_{2}^{2} v_{4} b_{1} + 32 v_{3}^{2} v_{4} b_{1} + 32 v_{4}^{3} b_{1} \\
128 v_{1}^{2} v_{4} b_{1} + 128 v_{2}^{2} v_{4} b_{1} + 64 v_{3}^{2} v_{4} b_{1} + 64 v_{4}^{3} b_{1} \\
32 v_{1}^{2} v_{4} b_{1} + 96 v_{2}^{2} v_{4} b_{1} + 32 v_{3}^{2} v_{4} b_{1} + 32 v_{4}^{3} b_{1} \\
128 v_{1}^{2} v_{4} b_{1} + 64 v_{2}^{2} v_{4} b_{1} + 128 v_{3}^{2} v_{4} b_{1} + 64 v_{4}^{3} b_{1} \\
32 v_{1}^{2} v_{3} b_{1} + 32 v_{2}^{2} v_{3} b_{1} + 32 v_{3}^{3} b_{1} + 96 v_{3} v_{4}^{2} b_{1} \\
64 v_{1}^{2} v_{4} b_{1} + 128 v_{2}^{2} v_{4} b_{1} + 128 v_{3}^{2} v_{4} b_{1} + 64 v_{4}^{3} b_{1} \\
32 v_{1}^{2} v_{4} b_{1} + 32 v_{2}^{2} v_{4} b_{1} + 96 v_{3}^{2} v_{4} b_{1} + 32 v_{4}^{3} b_{1} \\
32 v_{1}^{2} v_{2} c_{1} + 32 v_{2}^{3} c_{1} + 32 v_{2} v_{3}^{2} c_{1} + 96 v_{2} v_{4}^{2} c_{1} - 64 E v_{2} a_{1} \\
128 v_{2} v_{3} v_{4} b_{1} \\
128 v_{1} v_{3} v_{4} b_{1} \\
32 v_{1}^{2} v_{2} b_{1} + 32 v_{2}^{3} b_{1} + 32 v_{2} v_{3}^{2} b_{1} + 96 v_{2} v_{4}^{2} b_{1} \\
128 v_{1}^{2} v_{4} c_{1} + 128 v_{2}^{2} v_{4} c_{1} + 64 v_{3}^{2} v_{4} c_{1} + 64 v_{4}^{3} c_{1} - 128 E v_{4} a_{1} \\
128 v_{1}^{2} v_{4} c_{1} + 64 v_{2}^{2} v_{4} c_{1} + 128 v_{3}^{2} v_{4} c_{1} + 64 v_{4}^{3} c_{1} - 128 E v_{4} a_{1} \\
64 v_{1}^{2} v_{4} c_{1} + 128 v_{2}^{2} v_{4} c_{1} + 128 v_{3}^{2} v_{4} c_{1} + 64 v_{4}^{3} c_{1} - 128 E v_{4} a_{1} \\
64 v_{2} v_{4} c_{0} - 64 v_{2} a_{1} \\
-64 v_{3} v_{4} a_{1} + 64 v_{3} v_{4} b_{0} \\
-64 v_{2} v_{4} a_{1} + 64 v_{2} v_{4} b_{0} \\
32 v_{1}^{3} b_{1} + 32 v_{1} v_{2}^{2} b_{1} + 32 v_{1} v_{3}^{2} b_{1} + 96 v_{1} v_{4}^{2} b_{1} \\
128 v_{2} v_{3} v_{4} c_{1}
\end{array}
\end{equation}
\end{comment}

% This is the system of 34 equations from the older version of the code
% that doesn't seem to be consistently ordered (it changes every time I run the script)
\begin{comment}
\begin{equation}
% sage: load('helium.sage')
% sage: prep_hydrogen(5)
% sage: init()
% sage: latex(matrix(eqns_RQQ).transpose())
% then copy-and-paste
\label{polynomial system}
\begin{array}{r}
4 v_{1} v_{3} v_{4} b_{1} \\
v_{1}^{2} v_{2} b_{1} + v_{2}^{3} b_{1} + v_{2} v_{3}^{2} b_{1} + 3 v_{2} v_{4}^{2} b_{1} \\
-2 v_{4} a_{0} \\
4 v_{1}^{2} v_{4} b_{1} + 4 v_{2}^{2} v_{4} b_{1} + 2 v_{3}^{2} v_{4} b_{1} + 2 v_{4}^{3} b_{1} \\
3 v_{1}^{2} v_{4} b_{1} + v_{2}^{2} v_{4} b_{1} + v_{3}^{2} v_{4} b_{1} + v_{4}^{3} b_{1} \\
v_{1}^{2} v_{3} b_{1} + v_{2}^{2} v_{3} b_{1} + v_{3}^{3} b_{1} + 3 v_{3} v_{4}^{2} b_{1} \\
2 v_{1}^{2} v_{4} c_{1} + 4 v_{2}^{2} v_{4} c_{1} + 4 v_{3}^{2} v_{4} c_{1} + 2 v_{4}^{3} c_{1} - 4 E v_{4} a_{1} \\
4 v_{1} v_{3} v_{4} c_{1} \\
v_{1}^{2} v_{3} c_{1} + v_{2}^{2} v_{3} c_{1} + v_{3}^{3} c_{1} + 3 v_{3} v_{4}^{2} c_{1} - 2 E v_{3} a_{1} \\
2 v_{2} v_{4} c_{0} - 2 v_{2} a_{1} \\
-2 v_{2} v_{4} a_{1} + 2 v_{2} v_{4} b_{0} \\
4 v_{1}^{2} v_{4} c_{1} + 4 v_{2}^{2} v_{4} c_{1} + 2 v_{3}^{2} v_{4} c_{1} + 2 v_{4}^{3} c_{1} - 4 E v_{4} a_{1} \\
3 v_{1}^{2} v_{4} c_{1} + v_{2}^{2} v_{4} c_{1} + v_{3}^{2} v_{4} c_{1} + v_{4}^{3} c_{1} - 2 E v_{4} a_{1} \\
v_{1}^{2} v_{4} c_{1} + 3 v_{2}^{2} v_{4} c_{1} + v_{3}^{2} v_{4} c_{1} + v_{4}^{3} c_{1} - 2 E v_{4} a_{1} \\
4 v_{1}^{2} v_{4} c_{1} + 2 v_{2}^{2} v_{4} c_{1} + 4 v_{3}^{2} v_{4} c_{1} + 2 v_{4}^{3} c_{1} - 4 E v_{4} a_{1} \\
4 v_{2} v_{3} v_{4} c_{1} \\
v_{1}^{2} v_{4} c_{1} + v_{2}^{2} v_{4} c_{1} + 3 v_{3}^{2} v_{4} c_{1} + v_{4}^{3} c_{1} - 2 E v_{4} a_{1} \\
2 v_{3} v_{4} c_{0} - 2 v_{3} a_{1} \\
-2 a_{0} \\
-2 v_{4}^{2} a_{1} + v_{1}^{2} b_{0} + v_{2}^{2} b_{0} + v_{3}^{2} b_{0} + v_{4}^{2} b_{0} \\
2 v_{1}^{2} v_{4} b_{1} + 4 v_{2}^{2} v_{4} b_{1} + 4 v_{3}^{2} v_{4} b_{1} + 2 v_{4}^{3} b_{1} \\
v_{1}^{2} v_{4} b_{1} + v_{2}^{2} v_{4} b_{1} + 3 v_{3}^{2} v_{4} b_{1} + v_{4}^{3} b_{1} \\
4 v_{1} v_{2} v_{4} c_{1} \\
v_{1}^{3} b_{1} + v_{1} v_{2}^{2} b_{1} + v_{1} v_{3}^{2} b_{1} + 3 v_{1} v_{4}^{2} b_{1} \\
-2 v_{3} v_{4} a_{1} + 2 v_{3} v_{4} b_{0} \\
v_{1}^{2} c_{0} + v_{2}^{2} c_{0} + v_{3}^{2} c_{0} + v_{4}^{2} c_{0} - 2 E a_{0} - 2 v_{4} a_{1} \\
2 v_{1} v_{4} c_{0} - 2 v_{1} a_{1} \\
v_{1}^{2} v_{2} c_{1} + v_{2}^{3} c_{1} + v_{2} v_{3}^{2} c_{1} + 3 v_{2} v_{4}^{2} c_{1} - 2 E v_{2} a_{1} \\
-2 v_{1} v_{4} a_{1} + 2 v_{1} v_{4} b_{0} \\
v_{1}^{2} v_{4} b_{1} + 3 v_{2}^{2} v_{4} b_{1} + v_{3}^{2} v_{4} b_{1} + v_{4}^{3} b_{1} \\
v_{1}^{3} c_{1} + v_{1} v_{2}^{2} c_{1} + v_{1} v_{3}^{2} c_{1} + 3 v_{1} v_{4}^{2} c_{1} - 2 E v_{1} a_{1} \\
4 v_{1} v_{2} v_{4} b_{1} \\
4 v_{1}^{2} v_{4} b_{1} + 2 v_{2}^{2} v_{4} b_{1} + 4 v_{3}^{2} v_{4} b_{1} + 2 v_{4}^{3} b_{1} \\
4 v_{2} v_{3} v_{4} b_{1}
\end{array}
\end{equation}
\end{comment}

\subsection{Prime Decomposition}

We now form
an ideal $I$ in $\mathbb{Q}[a_0,...,v_4,E]$ from \eqref{polynomial system}, and
compute its associated prime ideals:

\newcommand{\Bold}[1]{\mathbb{#1}}

\begin{subequations}
\label{ideal}
%sage: latex(I.minimal_associated_primes())
\begin{align}
% sage: sorted([latex(tuple(P.gens())) for P in I.minimal_associated_primes()])
% remove the list brackets and commas, add & at the beginning of each line, add \label and \\ at end of each line, normalize a fraction
& \left(a_{0}, v_{4}, v_{3}, v_{2}, v_{1}\right)\label{ideal:5} \\
& \left(c_{1}, c_{0}, b_{1}, b_{0}, a_{1}, a_{0}\right)\label{ideal:4} \\
& \left(v_{1}^{2} + v_{2}^{2} + v_{3}^{2}, a_{1}, a_{0}, v_{4}\right)\label{ideal:3} \\
& \left(v_{4} c_{0} - b_{0}, E c_{0} - v_{4} c_{1}, -v_{4}^{2} c_{1} + E b_{0}, b_{1}, 2 a_{1} - b_{0}, a_{0}, v_{3}, v_{2}, v_{1}\right)\label{ideal:2} \\
& \left(v_{4} c_{0} - b_{0}, v_{1}^{2} + v_{2}^{2} + v_{3}^{2} - v_{4}^{2}, c_{1}, b_{1}, a_{1} - b_{0}, a_{0}, E\right)\label{ideal:1}
\end{align}
\end{subequations}

Several of these varieties solve the system of equations, but do not lead to a meaningful solution to the differential equation.
In brief,

\begin{itemize}
\item[\eqref{ideal:5}] sets the variable $v$ to zero, so we discard it,
\item[\eqref{ideal:4}] sets all coefficients of the ODE to zero, so we discard it,
\item[\eqref{ideal:3}] sets the coefficient of
%$\frac{d^2\Psi}{dv^2}$
%$D^2\Psi$
$\Psi''$
in the ODE to zero, resulting in a first-order ODE, so we discard it, too,
\item[\eqref{ideal:2}] corresponds to the classical solutions (see below), and
\item[\eqref{ideal:1}] gives us a new solution (see below).

\end{itemize}

How to understand ideal \eqref{ideal:2}?  $a_0$ is an ideal generator,
so $a_0=0$, and we can always multiply our homogenous differential polynomial by a constant without affecting our result, so we can set $a_1=1$.
Likewise, we can multiply our variable $v$ by a constant and that will only change our coefficients by constants,
and $v_1=v_2=v_3=0$, so we can normalize by setting $v_4=1$.  This immediately implies that $b_0 = c_0 = 2$ and simplifies ideal \eqref{ideal:2} to:

\begin{equation}
\left(v_{3}, v_{2}, v_{1}, v_{4} - 1, c_{0} - 2, 2 E - c_{1}, b_{1}, 2 - b_{0}, a_{1} - 1, a_{0}\right)
\end{equation}

\begin{equation}
\label{classical eq in ideal}
\begin{gathered}
v=r \\
v \Psi'' + 2 \Psi' + 2(1 + E v) \Psi = 0
\end{gathered}
\end{equation}

This is the classical radial equation obtained by seperation of variables (\cite{pauling and wilson} \S18a).
%% or \url{http://hyperphysics.phy-astr.gsu.edu/hbase/quantum/hydrad.html}
We use spherical coordinates, set $\Psi = R(r)P(\theta)F(\psi)$, and obtain the above equation for $R(r)$,
though it is more commonly written in this form:

\begin{equation}
\frac{1}{R} \frac{d}{dr}\left[ r^2 \frac{dR}{dr}\right] + 2(Er^2 + r) = l(l+1)
\end{equation}

where $l$ is the orbital quantum number, which can be zero.  Set $l=0$, $v=r$ and $\Psi=R$ and
expand out the derivative to obtain \eqref{classical eq in ideal}.  This solution has been
known for a hundred years; its presence here was expected and won't be commented on further.

Ideal \eqref{ideal:1} was not expected, and contains a previously unknown solution.
$a_0$ is an ideal generator,
so $a_0=0$, and we can set $a_1=1$, by the same logic as above.
If we limit our $v_i$ coefficients to be real, all of their squares must
be positive, and since $v_1^2+v_2^2+v_3^3-v_4^2=0$, $v_4$ must be non-zero.  So we can normalize by setting $v_4=1$.
Substituting in these values, we find ideal generators that imply $b_0=1$ and simplify \eqref{ideal:1} to this ideal:

\begin{equation}
\left(v_{1}^{2} + v_{2}^{2} + v_{3}^{2} - 1, v_{4} - 1, c_{0} - 1, c_{1}, b_{1}, 1 - b_{0}, a_1 - 1, a_{0}, E\right)
\end{equation}

This ideal corresponds to the following system of equations:

\begin{equation}
\begin{gathered}
E = 0 \\
a_0 = 0 \qquad
a_1 = 1 \\
b_0 = -1 \qquad
b_1 = 0 \\
c_0 = -1 \qquad
c_1 = 0 \\
v_1^2 + v_2^2 + v_3^2 = 1 \qquad
v_4 = 1
\end{gathered}
\end{equation}

Substituting these values back into our ansatz, we conclude that $\Psi(v)$
is a solution of \eqref{schrodinger hydrogen} under these conditions:

\begin{equation}
\label{related solution}
\begin{gathered}
% v \frac{\delta^2\Psi}{\delta v^2} + \frac{\delta\Psi}{\delta v} + \Psi = 0 \\
v \Psi'' + \Psi' + \Psi = 0 \\
v = v_1 x+ v_2 y+ v_3 z+r \\
v_1^2 + v_2^2 + v_3^2 = 1
\end{gathered}
\end{equation}

The expression $v_1 x + v_2 y + v_3 z$ is easily identified as a dot product between
the coordinate $(x,y,z)$ and the unit vector $(v_1, v_2, v_3)$ (remembering
that $v_1^2 + v_2^2 + v_3^2 = 1$).  The direction of this vector is arbitrary,
so we can orient the $x$-axis in this direction and set $(v_1, v_2, v_3) = (1,0,0)$
without loss of generality.

We now have to solve a second order ODE:

\begin{equation}
v \Psi''(v) + \Psi'(v) + \Psi(v) = 0
\end{equation}

Wolfram Mathematica \cite{mathematica} can now
analyze this equation and determine that it is equivalent
to the Bessel function \eqref{solution}.

\begin{scriptsize}
\begin{verbatim}
In[1]:= DSolve[x*y''[x] + y'[x] + y[x] == 0, y[x], x]

Out[1]= {{y[x] -> BesselJ[0, 2 Sqrt[x]] C[1] + 2 BesselY[0, 2 Sqrt[x]] C[2]}}
\end{verbatim}
\end{scriptsize}

leading to the following exact solution to \eqref{schrodinger hydrogen}, with $E=0$:

\begin{equation}
\label{solution}
\Psi = J_0(2\sqrt{x+r})
\end{equation}

where $x,y,z$ are Cartesian coordinates, $r=\sqrt{x^2+y^2+z^2}$,
and $J_0$ is the ordinary Bessel function $J_0$.

Note that this solution could not be obtained by the method of separation of variables, as it is non-separable
(i.e, it can not be written in the form $R(r)\Theta(\theta)\Psi(\psi)$ or $X(x)Y(y)Z(z)$).

% \vskip 12pt

%Then \eqref{solution} is an exact solution to \eqref{schrodinger}, with $E=0$.

%\subsection*{Verification}

\label{verification}
%The result can be easily verified using Mathematica\footnote{A manual verification is presented in an appendix}, as follows:
The result can be easily verified using Wolfram Mathematica \cite{mathematica}, as follows:

\begin{scriptsize}
\begin{verbatim}
In[1]:= Psif := Psi[x, y, z]

In[2]:= r[x_, y_, z_] = Sqrt[x^2 + y^2 + z^2]

              2    2    2
Out[2]= Sqrt[x  + y  + z ]

In[3]:= eqn = - 1 / 2 * Laplacian[Psif, {x, y, z}] - 1 / r[x, y, z] * Psif == 0

                                    (0,0,2)               (0,2,0)               (2,0,0)
             Psi[x, y, z]       -Psi       [x, y, z] - Psi       [x, y, z] - Psi       [x, y, z]
Out[3]= -(------------------) + ---------------------------------------------------------------- == 0
                2    2    2                                    2
          Sqrt[x  + y  + z ]

In[4]:= sol[x_, y_, z_] := BesselJ[0, 2 * Sqrt[x + r[x, y, z]]]

In[5]:= FullSimplify[eqn /. Psi -> ({x, y, z} |-> sol[x, y, z])]

Out[5]= True
\end{verbatim}
\end{scriptsize}

\subsection{Physical Interpretation}

Since \eqref{solution} is a solution to a simple Schr\"odinger equation for hydrogen,
it is reasonable to ask if it has any physical interpretation.  It does not.

In order to be a physically realistic solution, a function must both solve the Schr\"odinger equation
and be a valid wavefunction.  For a given wavefunction $\Psi$, the modulus squared $|\Psi|^2$ is a probability density function
for the position of the electron (\cite{pauling and wilson} \S 10a),
%and must have a finite integral,
so it follows that a valid
wavefunction must be in $L^2(\mathbb{R}^3)$, since its total probability must be finite.

%Bessel's original definition of $J_0$ (\cite[p. 19]{Watson}; \cite[\href{https://dlmf.nist.gov/10.9.E1}{Eq (10.9.1)}]{DLMF}) is:
Bessel's original definition of $J_0$ \cite[\href{https://dlmf.nist.gov/10.9.E1}{Eq (10.9.1)}]{DLMF} is:

\begin{equation}
\label{J0 definition}
J_0(z) = \frac{1}{2\pi}\int_0^{2\pi} \cos(z\sin\theta) d\theta
\end{equation}

% Bessel's integral form for $J_0$ is ; for its graph see \cite[\href{https://dlmf.nist.gov/10.3.F1}{Figure 10.3.1}]{DLMF}.

$J_0(z)$ is continuous everywhere on the real line and $J_0(0)=1$.  Also, $2\sqrt{x+r}$ is continuous
everywhere in $\mathbb{R}^3$ and is zero along the negative $x$-axis.  Therefore, there is a small
tube around the negative $x$-axis where $J_0(2\sqrt{x+r}) \approx 1$.
This tube has positive measure and infinite extent, so $J_0(2\sqrt{x+r}) \notin L^2(\mathbb{R}^3)$,
% and \eqref{solution}
is not valid a wavefunction and does not correspond to any physically realized quantum mechanical state.

% These facts imply that for any
% $\delta>0$ there exists an $\epsilon>0$ such that $J_0(2\sqrt{x+r}) > 1-\epsilon$ for any point within
% a distance of $\delta$ from the negative $x$-axis. (uniform continuity?)

% Equation \eqref{solution} is not in $L^2(\mathbb{R}^3)$.  $J_0(0)=1$, as can be seen from both
% its integral form (\cite[\href{https://dlmf.nist.gov/10.9.E1}{Eq (10.9.1)}]{DLMF}) and its graph (\cite[\href{https://dlmf.nist.gov/10.3.F1}{Figure 10.3.1}]{DLMF}).
% Therefore, equation \eqref{solution} is 1 along the negative $x$-axis, which means these solutions
% are not valid wavefunctions and do not, therefore, correspond to any physically realized quantum mechanical state.

This situation is not unusual.  Classical solutions to hydrogen exist (as hypergeometric series) for all energy values,
but these solutions are only in $L^2(\mathbb{R}^3)$ for specific values of $E$;
only these wavefunctions are observed as atomic orbitals.

\subsection{Generalization of the Example}
\parskip 12pt

The choice of $x$ is arbitrary, and any ordinary Bessel function can be used:

\begin{equation}
\label{generalized solution}
\Psi = F(2\sqrt{a_1 x+ a_2 y+ a_3 z+r})
\end{equation}

where

\begin{equation*}
a_1^2+a_2^2+a_3^2=1
\end{equation*}

and $F$ is any linear combination of the Bessel functions $J_0$ and $Y_0$.

Futhermore, since wavefunctions are complex-valued, restricting the $v_i$ coefficients
to be real is unnecessary.

Any finite linear combination of functions of the form \eqref{generalized solution} also solves \eqref{schrodinger hydrogen}.

\section{Generalization of the Algorithm}

More complex ansatzen can be constructed.  For example, here is an ansatz that introduces the square root
of a first degree polynomial, then allows the ODE's independent variable to be selected from the resulting
algebraic extension:

\begin{center}
\begin{tikzpicture}[remember picture,out=315,in=225,distance=0.4cm,node distance=40pt]
  \node (Psi) [poly, text width=100pt] {$\framebox(10,10){}\tikzmark{a}\,\Psi'' + \framebox(10,10){}\tikzmark{b}\,\Psi' + \coeff\tikzmark{c}\,\Psi$};
  \node (Qv) [ring, below of=Psi, text width=100pt] {$\mathbb{Q}[v]$};
  \node (v) [poly, right=of Qv] {$v$};
  \draw[degree] (a.west) -- (a.west|-Qv.north) node[right,pos=0.6] {1};
  \draw[degree] (b.west) -- (b.west|-Qv.north) node[right,pos=0.6] {1};
  \draw[degree] (c.west) -- (c.west|-Qv.north) node[right,pos=0.6] {1};
  \node (Psi label) [at=(v.east|-Psi.north), anchor=north east] {\Large$\Psi$};
  \begin{scope}[on background layer]
      \node (Psi block)[fit=(Psi) (Qv) (v), inner sep=10pt, element] {};
  \end{scope}
  \node (alg ext) [poly, node distance=150pt, below=of Psi.east, anchor=east, text width=100pt] {$\gamma^2 = \framebox(10,10){}\tikzmark{a}$};
  \begin{scope}[on background layer]
      \node (alg ext element) [algebraic, fit=(alg ext) (Psi label.east|-alg ext.north), inner sep=10pt] {};
      \node (alg ext title) [node distance=20pt, above=of alg ext element.north, anchor=center, text centered, text width=250pt] {$\mathbb{Q}[x,y,z,r,\gamma]/(r^2-x^2-y^2-z^2, \gamma^2 - \coeff\,)$};
      \node (alg ext field) [ring, fill=none, fit=(alg ext title) (alg ext element), node distance=70pt, text width=250pt] {};
  \end{scope}
  \draw[degree] (v.south) -- (v.south|-alg ext field.north) node[right,pos=0.7] {1};
  \node (base) [ring, below=of alg ext field.south east, anchor=east, text width=250pt] {$\mathbb{Q}[x,y,z,r]/(r^2-x^2-y^2-z^2)$};
  \draw[degree] (a.west) -- (a.west|-base.north) node[right,pos=0.8] {1};
\end{tikzpicture}
\end{center}


\begin{equation}
\label{ansatz 16.1}
\begin{gathered}
\Psi_x = \Psi' v_x \qquad
\Psi_y = \Psi' v_y \qquad
\Psi_z = \Psi' v_z \\
\Psi'_x = \Psi'' v_x \qquad
\Psi'_y = \Psi'' v_y \qquad
\Psi'_z = \Psi'' v_z \\
(a_0 + a_1 v) \Psi'' + (b_0 + b_1 v) \Psi' + (c_0 + c_1 v) \Psi = 0 \\
v = v_1 x + v_2 y + v_3 z + v_4 r + v_5 \gamma \\
\gamma^2 = g_1 x + g_2 y + g_3 z + g_4 r + g_5
\end{gathered}
\end{equation}

It is also possible to introduce additional extensions into the univariate ring from which the ODE's coefficients
are selected, i.e.:

\begin{center}
  \begin{tikzpicture}[remember picture,out=315,in=225,distance=0.4cm,node distance=40pt]
    \node (Psi) [poly, text width=100pt] {$\coeff\tikzmark{a}\,\Psi'' + \coeff\tikzmark{b}\,\Psi' + \coeff\tikzmark{c}\,\Psi$};
    \node (Qvw) [ring, below of=Psi, text width=100pt] {$\mathbb{Q}[v,w]$ \vskip 5pt $\coeff\tikzmark{wa}\,w'' + \coeff\tikzmark{wb}\,w' + \coeff\tikzmark{wc}\,w$};
    \node (Qv) [ring, below of=Qvw, text width=100pt] {$\mathbb{Q}[v]$};
    \node (v) [poly, right=of Qv] {$v$};
    \draw[degree] (a.west) -- (a.west|-Qvw.north) node[right,pos=0.6] {1};
    \draw[degree] (b.west) -- (b.west|-Qvw.north) node[right,pos=0.6] {1};
    \draw[degree] (c.west) -- (c.west|-Qvw.north) node[right,pos=0.6] {1};
    \draw[degree] (wa.west) -- (wa.west|-Qv.north) node[right,pos=0.6] {1};
    \draw[degree] (wb.west) -- (wb.west|-Qv.north) node[right,pos=0.6] {1};
    \draw[degree] (wc.west) -- (wc.west|-Qv.north) node[right,pos=0.6] {1};
    \node (Psi label) [at=(v.east|-Psi.north), anchor=north east] {\Large$\Psi$};
    \begin{scope}[on background layer]
        \node (Psi block)[fit=(Psi) (Qv) (Qvw) (v), inner sep=10pt, element] {};
    \end{scope}
    \node (base) [ring, node distance=20pt, below=of Psi block.south, anchor=north, text width=250pt] {$\mathbb{Q}[x,y,z,r]/(r^2-x^2-y^2-z^2)$};
    \draw[degree] (v.south) -- (v.south|-base.north) node[right,pos=0.7] {1};
  \end{tikzpicture}
\end{center}

\begin{equation}
\label{ansatz 17}
\begin{gathered}
\Psi_x = \Psi' v_x \qquad
\Psi_y = \Psi' v_y \qquad
\Psi_z = \Psi' v_z \\
\Psi'_x = \Psi'' v_x \qquad
\Psi'_y = \Psi'' v_y \qquad
\Psi'_z = \Psi'' v_z \\
(a_0 + a_1 v + a_2 w) \Psi'' + (b_0 + b_1 v + b_2 w) \Psi' + (c_0 + c_1 v + c_2 w) \Psi = 0 \\
(d_0 + d_1 v) w'' + (e_0 + e_1 v) w' + (f_0 + f_1 v) w = 0 \\
v = v_1 x + v_2 y + v_3 z + v_4 r
\end{gathered}
\end{equation}

Algebraic extension.

Polynomials of any (finite) degree can be used in lieu of the linear polynomials.

Generalization of the ODE extension. Non-linear and inhomogeneous ODEs can be used to construct the ansatzen, as follows.

Non-linear ODEs.  For a linear homogeneous second-order ODE, we have $\Psi''$, $\Psi'$, and $\Psi$,
multiplied by polynomials in $v$.  Our differential polynomial is first degree in $\Psi'', \Psi', \Psi$.
For a non-linear ODE, we can introduce higher degrees.  For example, a second
degree second order ODE would add $\Psi''^2$, $\Psi''\Psi'$, $\Psi''\Psi$, $\Psi'^2$, $\Psi'\Psi$, and $\Psi^2$ to the ansatz.

Inhomogenous ODEs.  Just like the non-linear case, a new polynomial in $v$, which could be seen a zero-th degree in $\Psi,\Psi',\Psi''$.

Intermediate ODE used to construct the coefficient ring of the ODE proper.

Parameterized ODEs.  We could allow coefficients in the ODE to involve indeterminates other than $v$,
so long as they overlap with any of the indeterminates in $v$, or intersect with anything like
an algebraic extension ($r^2-x^2-y^2-z^2$) that would connect variables together.  In short,
there would need to be two independent sets of indeterminates, one that $v$ is expressed in terms of,
and one that the coefficients of the ODE are expressed in terms of.  Then we would have to solve
a parameterized ODE, one whose coefficients varied according to one set of independent variables,
and whose independent variable depended on a different set on them.  The advantage being that
the parameters can be treated as constants for the purpose of solving the ODE.
I don't know of any way to enforce this requirement (allowing $v$ to
depend on one set of indeterminates and allowing another set into the coefficients),
other than having a bunch of different ansatzen to test, but of course one large ansatz could
be constructed that would enforce nothing, but would still allow for ODE solutions of the form described next.

What if the dependencies of the ODE coefficients and the ODE variable can't be separated into two different sets of variables?

Then we're going to get an ODE something like this (two dimensional, for illustrative purposes):

\begin{equation}
\label{weird ODE}
f(u,v) \frac{d^2\Psi}{dv^2} + g(u,v) \frac{d\Psi}{dv} + h(u,v) \Psi = 0
\end{equation}

the point being that we can't separate the $v$ dependency out of the coefficients.

I'm not aware of any theory to handle an ODE of the form \eqref{weird ODE}, but one might exist,
or be developed.  Would it be useful?  Perhaps.  \eqref{weird ODE} is still simpler than a PDE because
it only involves one derivative.  The algorithm described in this paper could test for the existence
of such solutions.  I'm somewhat skeptical of their utility.

\section{Selection of an Ansatz}

The most obvious question remains: how to select an ansatz?

I do not have any very good answer to that question.  The ansatz used for the hydrogen example
was an educated guess.  It was designed to be general enough to find the existing
classical solution, to be used for testing the software.  The appearance of \eqref{ideal:1}
was not anticipated.

Nevertheless, let me comment briefly on the theoretical picture.

Picard-Vessiot theory.
Great progress has been made since the time of Kolchin in analyzing ordinary
differential equations.  An $n$-order linear ODE,
restricted to analytic solutions (complex? what function space?), will have at most an $n$-dimensional
solution space.  After selecting an arbitrary basis, all of these
solutions can be represented by an $n$-tuple of complex constants.
Any automorphism (permutation?) of these solutions can be represented by
an $n \times n$ matrix, so differential Galois theory can be reduced
to the analysis of $GL_n(\mathbb{C})$, which has facilitated the work of Kovacic, Singer, and Bronstein.

No such easy reduction is available for PDEs.
A PDE with no boundary conditions
is likely to have a solution space that is not only infinite, but infinite
dimensional, and probably not even with a countably infinite basis.
In such an environment, differential Galois groups
can not be represented as subgroups of $GL_n$,
so the theory of algebraic groups is therefore unlikely to be much help.
Furthermore, note that Kovacic developed an algorithm to handle
second order ODEs not by developing an algorithm to determine the
differential Galois group of a given DE, but rather by
categorizing all possible algebraic subgroups of $SL_2$
and developing an algorithm to handle each case.
Taking a similar approach for PDEs would require categorizing
all possible linear mappings between function spaces,
which does not seem feasible.

\section{Software}

The script used to perform the calculations in this paper is available here:

\centerline{\url{https://github.com/BrentBaccala/helium}}

{\tt joca.sage} is a SageMath script, verified to work with SageMath 10.6.

SageMath \cite{sage} integrates dozens of open source mathematical programs and libraries into
a unified, Python-based framework.  For this calculation, it uses Fran\c{c}ois Boulier's
{\tt DifferentialAlgebra} package \cite{DifferentialAlgebra} to perform the Ritt reduction
step, and Singular \cite{DGPS}'s implementation of the GTZ algorithm \cite{GTZ}
to compute the associated prime ideals of \eqref{polynomial system}

%%\section*{Current Implementation Status}
%%\subsection{Current Implementation Status}
\section{Future Work}

The difficulties of selecting an ansatz have already be touched upon.
One possible approach to ansatz selection would be to impose boundary
conditions (or the like) prior to any kind of analysis with differential algebra.
In the case of quantum mechanics, imposing $H^2(\Omega)$, where $\Omega$ is the state space
of whatever physical system we're analyzing, is an obvious restriction.
Some kind of theoretical analysis, exploring what restrictions must be placed
on ansatzen to force the functions into $H^2(\Omega)$, might yield
useful techniques to better select ansatzen.

What relationship is there between the constant ideals and the function space?

The ranking was selected in somewhat of an ad hoc manner.  A more systematic
method of analyzing an ansatz to determine a suitable ranking would be useful.

While discarding ideals \eqref{ideal:5} and \eqref{ideal:4} seems justified
(the independent variable must not be zero, and the ODE must have some non-zero coefficients),
discarding ideal \eqref{ideal:3} (setting the ODE's second-order term to zero)
seems somewhat arbitrary.  We would like to
analyze the first-order ODE case as a special case of the second-order ODE.
The Rosenfeld-Gr\"obner algorithm \cite{blop} may provide a suitable tool for such
an analysis, as it decomposes a system of differential polynomials into
a family of overlapping systems with regularity properties that include requiring
the {\it initials} of the differential polynomials to be non-zero.  For an $n$-order ODE,
requiring its initial to be non-zero would require its $n$-th order term to be non-zero,
so Rosenfeld-Gr\"obner should split \eqref{ideal:3} into a separate system.
Unfortunately, our current implementation of Rosenfeld-Gr\"obner (\cite{DifferentialAlgebra})...

The utility of this algorithm is currently limited by our Gr\"obner basis software.
For example, I have formed a system of 1570 equations with 1,296,960 terms
to test whether helium can be solved using a second-order ODE with second
degree coefficients and a second degree independent variable.  Various
attempts to compute a Gr\"obner basis for this system exhaust the
memory on a computer with 96 GB RAM, but the software lacks the ability to
fall back effectively on mass storage when RAM is exhausted, nor
can it use memory spread out over a cluster.

I've developed working software that can parallize the computation
of the minimal associated primes through factorization and monotone dualization.
It's inefficient, which is a major drawback for me, as my compute resources
are somewhat limited.  I've been devoting more of my recent efforts towards more
efficient algorithms.

\section{Conclusion}

The algorithm presented above can be used to check any PDE to see if any of its solutions
can be expressed using an ODE structured according to a specific ansatz, defined by
differential polynomials and parameterized by a finite number of constants.
This technique
is complementary to separation of variables, where we check a PDE to see if any of
its solutions can be expressed as a product of factors, each depending on only
a single variable, with no restrictions to differential polynomials.

The name of the software repository ({\tt helium}) informs a primary goal
of my research: to find a closed form expression for helium's ground state.
In that repository, you will find several years of fumbling attempts in
that direction, of which {\tt joca.sage} is the most refined.
\footnote{{\tt helium.sage} is also notable for containing a very aggressive, though inefficient,
factorizing implementation of the GTZ algorithm, but that's the subject of another paper.}
Although ``solving helium'' is widely regarded as impossible, I know
of no proof that helium's ground state wavefunction can not be formed
from a finite number of linear homogenous ODEs.  Lacking any good theory
to inform the selection of an ansatz, I have turned to...

Lacking a solution to helium, we can explore other important PDEs.  The Navier-Stokes
equation comes to mind.  I'd like to see an ansatz like this explored for Navier-Stokes:

[insert diagram]


As with helium, we can work incrementally towards it using smaller ansatzen.

Perhaps the biggest takeaway message is that we may now face a situation
similar to that of the big AI companies, once they had developed transfomer
architectures and it was clear that a massively expensive computation effort
was likely to yield spectacular results.

We can solve differential equations with Gr\"obner bases,
and we may even be on the verge of solving helium and/or Navier-Stokes
but we're primarily limited by inefficienies of our algorithms


\begin{comment}
As my primary interest lies in quantum mechanics, I have investigated the PDEs
that model hydrogen and helium.

For hydrogen, the PDE is $\nabla^2 \Psi - \frac{1}{r} \Psi = E \Psi$

For helium, the PDE is $\nabla_1^2 \Psi + \nabla_2^2 \Psi - \frac{2}{r_1} - \Psi \frac{2}{r_2} - \Psi \frac{1}{r_{12}} \Psi = E \Psi$,
where $\Psi=\Psi(r_1,r_2,r_{12})$ and $\nabla_i$ is the Laplacian with respect to the $i^{\rm th}$ electron.
$\Psi$ is assumed to have no angular dependence, which has been known since
at least the time of Hylleraas to be a valid assumption for the ground state.

In both cases, I use Hartree atomic units to render the equations dimensionless.
\end{comment}

\begin{thebibliography}{9}

\bibitem{blop}
F. Boulier, D. Lazard, F. Ollivier and M. Petitot, “Computing representations for radicals
of finitely generated differential ideals”, Special issue “Jacobi’s Legacy” of AAECC, 20, (1), 73–121, 2009.

\bibitem{Czapor87}
S. R. Czapor. Solving algebraic equations via Buchberger’s algorithm. In Proc. EUROCAL’87,
volume 378 of Lect. Notes Comp. Sci., pages 260 -- 269, 1987.

\bibitem{Grabe95}
Hans-Gert Gr\"abe (1995). ``Triangular systems and factorized Gr\"obner bases''

\bibitem{Grabe96}
Hans-Gert Gr\"abe (1996). ``Minimal Primary Decomposition and Factorized Gr\"obner Bases''

\bibitem{Grabe06}
Gr\"abe (2006).  ``The Groebner Factorizer and Polynomial System Solving'',
Talk given at the Special Semester on Groebner Bases, Linz 2006.

\url{https://www.ricam.oeaw.ac.at/specsem/srs/groeb/download/06_02_Solver.pdf}

\bibitem{GTZ}
Gianni, P., Trager, B., Zacharias, G.: ``Gr\"obner bases and primary decomposition of polynomial ideals''.
{\it J. Symb. Comp.} 6 (1988), 149 - 167.

\bibitem{Ritt}
Joseph Fels Ritt. Differential Algebra. Dover Publications Inc., New York,
1950. \url{http://www.ams.org/online_bks/coll33}

\bibitem{Kolchin}
Ellis Robert Kolchin. Differential Algebra and Algebraic Groups. Academic
Press, New York, 1973.

\bibitem{DGPS}
Decker, W.; Greuel, G.-M.; Pfister, G.; Sch{\"o}nemann, H.:
\newblock {\sc Singular} {4-4-1} --- {A} computer algebra system for polynomial computations.
\newblock {\tt https://www.singular.uni-kl.de} (2025).

\bibitem{aistleitner}
Christian Aistleitner:
Relations between Gr\"obner bases, differential Gr\"obner bases, and differential characteristic sets.
Linz, Dezember 2010.
\url{https://www3.risc.jku.at/publications/download/risc_4229/master.pdf}

\bibitem{pauling and wilson}
Pauling, L., \& Wilson, E. B. (1935). {\it Introduction to Quantum Mechanics with Applications to Chemistry}. McGraw-Hill

\bibitem{mathematica}
Wolfram Research, Inc., Mathematica, Version 14.0, Champaign, IL (2023).

\bibitem{sage}
\emph{{S}ageMath, the {S}age {M}athematics {S}oftware {S}ystem ({V}ersion
  10.6)}, The Sage Developers, 2025, {\tt https://www.sagemath.org}.

\begin{comment}
M. Abramowitz and I. A. Stegun (Eds.) (1964) Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. National Bureau of Standards Applied Mathematics Series, U.S. Government Printing Office, Washington, D.C
\end{comment}


\bibitem{DLMF}
NIST Digital Library of Mathematical Functions. https://dlmf.nist.gov/, Release 1.2.5 of 2025-12-15. F. W. J. Olver, A. B. Olde Daalhuis, D. W. Lozier, B. I. Schneider, R. F. Boisvert, C. W. Clark, B. R. Miller, B. V. Saunders, H. S. Cohl, and M. A. McClain, eds.

\bibitem{Watson}
{\it A Treatise On The Theory Of Bessel Functions}
Watson.

\bibitem{iva}
Cox, D., Little, J., and O'Shea, D. (2015).
{\it Ideals, Varieties, and Algorithms: An Introduction to Computational Algebraic Geometry and Commutative Algebra}
(4th ed.). Springer

\bibitem{DifferentialAlgebra}
Fran\c{c}ois Boulier, \emph{DifferentialAlgebra} (Version 5.3), 2025.\\
{\tt https://codeberg.org/francois.boulier/DifferentialAlgebra}
\end{thebibliography}

\end{document}
