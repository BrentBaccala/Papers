
\documentclass{article}

\title{A New Solution of Hydrogen}
\author{Brent Baccala}

\usepackage{amsmath}

\usepackage{xcolor}
\usepackage{comment}
\usepackage{graphicx}

\usepackage[hidelinks]{hyperref}

\usepackage{tabularx}

\begin{document}
\parindent 0pt

\maketitle

\begin{abstract}
I show a previously unknown exact solution to the simplest time-independent Schr\"odinger equation for hydrogen.
The solution involves a Bessel function, is not separable, and is not in $L^2$.
\end{abstract}

\subsection*{Theorem}

Consider the following simple Sch\"odinger equation for the hydrogen atom:

\begin{equation}
\label{schrodinger}
-\frac{1}{2}\nabla^2 \Psi - \frac{1}{r}\Psi = E \Psi
\end{equation}

Let $J_0$ be the ordinary Bessel function $J_0$, and set

\begin{equation}
\label{solution}
\Psi = J_0(2\sqrt{x+r})
\end{equation}

where $x,y,z$ are Cartesian coordinates and $r=\sqrt{x^2+y^2+z^2}$.

\vskip 12pt

Then \eqref{solution} is an exact solution to \eqref{schrodinger}, with $E=0$.

\subsection*{Verification}

The result can be easily verified using Mathematica, as follows:

\includegraphics[page=1, clip, trim=1in 7in 1in 1in, width=\textwidth]{improved.pdf}

\vfill\eject
\subsection*{Proof}
The claim is that $\Psi = J_0(2\sqrt{x+r}) = (J_0 \circ 2\sqrt{v}) (x+r)$ satisfies:

\begin{equation}
\label{claim}
\left(\frac{\delta^2}{\delta^2 x} + \frac{\delta^2}{\delta^2 y} + \frac{\delta^2}{\delta^2 z}\right) \Psi + \frac{2}{r}\Psi = 0
\end{equation}

\vskip 12pt

Letting $v=x+r$, we compute the first partial derivatives of $\Psi$:

\begin{equation}
\begin{gathered}
\frac{\delta \Psi}{\delta x} = \frac{d v}{d x} \frac{d}{d v} \left(J_0 \circ 2\sqrt{v}\right) = \frac{d v}{d x} J_0'(2\sqrt{v}) v^{-1/2} \\
\frac{\delta \Psi}{\delta y} = \frac{d v}{d y} \frac{d}{d v} \left(J_0 \circ 2\sqrt{v}\right) = \frac{d v}{d y} J_0'(2\sqrt{v}) v^{-1/2} \\
\frac{\delta \Psi}{\delta z} = \frac{d v}{d z} \frac{d}{d v} \left(J_0 \circ 2\sqrt{v}\right) = \frac{d v}{d z} J_0'(2\sqrt{v}) v^{-1/2}
\end{gathered}
\end{equation}

\vskip 12pt

Next we compute the partial second derivatives of $\Psi$:

\begin{equation}
\label{second partials}
\begin{gathered}
\frac{\delta^2 \Psi}{\delta x^2} = \frac{d^2 v}{d x^2} J_0'(2\sqrt{v}) v^{-1/2} + \left(\frac{d v}{d x}\right)^2 J_0''(2\sqrt{v}) v^{-1} - \frac{1}{2} \left(\frac{d v}{d x}\right)^2 J_0'(2\sqrt{v}) v^{-3/2} \\
\frac{\delta^2 \Psi}{\delta y^2} = \frac{d^2 v}{d y^2} J_0'(2\sqrt{v}) v^{-1/2} + \left(\frac{d v}{d y}\right)^2 J_0''(2\sqrt{v}) v^{-1} - \frac{1}{2} \left(\frac{d v}{d y}\right)^2 J_0'(2\sqrt{v}) v^{-3/2} \\
\frac{\delta^2 \Psi}{\delta z^2} = \frac{d^2 v}{d z^2} J_0'(2\sqrt{v}) v^{-1/2} + \left(\frac{d v}{d z}\right)^2 J_0''(2\sqrt{v}) v^{-1} - \frac{1}{2} \left(\frac{d v}{d z}\right)^2 J_0'(2\sqrt{v}) v^{-3/2}
\end{gathered}
\end{equation}

\vskip 12pt

We need to know the derivatives of $v=r+x$ with respect to the coordinates:

\vskip 12pt

\begin{equation}
\label{first v}
\begin{gathered}
\frac{d v}{d x} = \frac{d}{d x} (x+r) = 1 + \frac{x}{r} \\
\frac{d v}{d y} = \frac{d}{d y} (x+r) = \frac{y}{r} \\
\frac{d v}{d z} = \frac{d}{d z} (x+r) = \frac{z}{r}
\end{gathered}
\end{equation}

\vskip 12pt

\begin{equation}
\label{second v}
\begin{gathered}
\frac{d^2 v}{d x^2} = \frac{d}{d x} \left(1 + \frac{x}{r}\right) = \frac{r - x(x/r)}{r^2} = \frac{r^2 - x^2}{r^3} \\
\frac{d^2 v}{d y^2} = \frac{r^2 - y^2}{r^3} \\
\frac{d^2 v}{d z^2} = \frac{r^2 - z^2}{r^3}
\end{gathered}
\end{equation}

\vskip 20pt

Substituting \eqref{first v} and \eqref{second v} into \eqref{second partials}, and \eqref{second partials}
into the LHS of \eqref{claim}, we obtain:

\begin{equation*}
\begin{aligned}
&\frac{d^2 v}{d x^2} J_0'(2\sqrt{v}) v^{-1/2} + \left(\frac{d v}{d x}\right)^2 J_0''(2\sqrt{v}) v^{-1} - \frac{1}{2} \left(\frac{d v}{d x}\right)^2 J_0'(2\sqrt{v}) v^{-3/2} \\
+& \frac{d^2 v}{d y^2} J_0'(2\sqrt{v}) v^{-1/2} + \left(\frac{d v}{d y}\right)^2 J_0''(2\sqrt{v}) v^{-1} - \frac{1}{2} \left(\frac{d v}{d y}\right)^2 J_0'(2\sqrt{v}) v^{-3/2} \\
+& \frac{d^2 v}{d z^2} J_0'(2\sqrt{v}) v^{-1/2} + \left(\frac{d v}{d z}\right)^2 J_0''(2\sqrt{v}) v^{-1} - \frac{1}{2} \left(\frac{d v}{d z}\right)^2 J_0'(2\sqrt{v}) v^{-3/2} \\
+& \frac{2}{r} J_0(2\sqrt{v})
\end{aligned}
\end{equation*}

\begin{equation*}
\begin{aligned}
=&\frac{r^2-x^2}{r^3} J_0'(2\sqrt{v}) v^{-1/2} + (1+\frac{x}{r})^2 J_0''(2\sqrt{v}) v^{-1} - \frac{1}{2} (1+\frac{x}{r})^2 J_0'(2\sqrt{v}) v^{-3/2} \\
&+ \frac{r^2-y^2}{r^3} J_0'(2\sqrt{v}) v^{-1/2} + \left(\frac{y}{r}\right)^2 J_0''(2\sqrt{v}) v^{-1} - \frac{1}{2} \left(\frac{y}{r}\right)^2 J_0'(2\sqrt{v}) v^{-3/2} \\
&+ \frac{r^2-z^2}{r^3} J_0'(2\sqrt{v}) v^{-1/2} + \left(\frac{z}{r}\right)^2 J_0''(2\sqrt{v}) v^{-1} - \frac{1}{2} \left(\frac{z}{r}\right)^2 J_0'(2\sqrt{v}) v^{-3/2} \\
&+ \frac{2}{r} J_0(2\sqrt{v})
\end{aligned}
\end{equation*}

\begin{equation*}
\begin{aligned}
=&\frac{3r^2-x^2-y^2-z^2}{r^3} J_0'(2\sqrt{v}) v^{-1/2} + (1+2\frac{x}{r} +\frac{x^2}{r^2} + \frac{y^2}{r^2} + \frac{z^2}{r^2}) J_0''(2\sqrt{v}) v^{-1} \\
&- \frac{1}{2} (1+2\frac{x}{r} +\frac{x^2}{r^2}+ \frac{y^2}{r^2} + \frac{z^2}{r^2}) J_0'(2\sqrt{v}) v^{-3/2}
+ \frac{2}{r} J_0(2\sqrt{v})
\end{aligned}
\end{equation*}

\begin{equation*}
=\frac{2}{r} J_0'(2\sqrt{v}) v^{-1/2} + (2+2\frac{x}{r}) J_0''(2\sqrt{v}) v^{-1} - \frac{1}{2} (2+2\frac{x}{r}) J_0'(2\sqrt{v}) v^{-3/2} + \frac{2}{r} J_0(2\sqrt{v})
\end{equation*}

\begin{equation*}
=\frac{2}{r} J_0'(2\sqrt{v}) v^{-1/2} + 2\frac{x+r}{r} J_0''(2\sqrt{v}) v^{-1} - \frac{x+r}{r} J_0'(2\sqrt{v}) v^{-3/2}
+ \frac{2}{r} J_0(2\sqrt{v})
\end{equation*}

Remembering that $v=x+r$,

%%\begin{equation*}
%%=\frac{2}{r} J_0'(2\sqrt{v}) v^{-1/2} + 2\frac{x+r}{r} J_0''(2\sqrt{v}) v^{-1} - \frac{1}{r} J_0'(2\sqrt{v}) v^{-1/2}
%%+ \frac{2}{r} J_0(2\sqrt{v})
%%\end{equation*}

\begin{equation*}
=\frac{2}{r} J_0'(2\sqrt{v}) v^{-1/2} + \frac{2}{r} J_0''(2\sqrt{v}) - \frac{1}{r} J_0'(2\sqrt{v}) v^{-1/2}
+ \frac{2}{r} J_0(2\sqrt{v})
\end{equation*}

\begin{equation*}
=\frac{2}{r} J_0''(2\sqrt{v}) + \frac{1}{r} J_0'(2\sqrt{v}) v^{-1/2} + \frac{2}{r} J_0(2\sqrt{v})
\end{equation*}

\begin{equation*}
=\frac{2}{r} J_0''(2\sqrt{v}) + \frac{2}{r\cdot2\sqrt{v}} J_0'(2\sqrt{v}) + \frac{2}{r} J_0(2\sqrt{v})
\end{equation*}

\begin{equation}
\label{last eq in derivation}
=\frac{2}{r} \left( J_0''(2\sqrt{v}) + \frac{1}{2\sqrt{v}} J_0'(2\sqrt{v}) + J_0(2\sqrt{v})\right)
\end{equation}

Now, the ordinary Bessel function $J_0(x)$ satisfies:

\begin{equation*}
x^2 J_0''(x) + xJ_0'(x) + x^2J_0(x) = 0
\end{equation*}

dividing through by $x^2$ and changing variables, we get:

\begin{equation*}
J_0''(2\sqrt{v}) + \frac{1}{2\sqrt{v}}J_0'(2\sqrt{v}) + J_0(2\sqrt{v}) = 0
\end{equation*}

which shows that \eqref{last eq in derivation} is zero, and establishes the proof of \eqref{claim}.

\subsection*{Generalization}
\parskip 12pt

The choice of $x$ is arbitrary, and any ordinary Bessel function can be used:

\begin{equation}
\label{generalized solution}
\Psi = F(2\sqrt{a_1 x+ a_2 y+ a_3 z+r})
\end{equation}

where

\begin{equation*}
a_1^2+a_2^2+a_3^2=1
\end{equation*}

and $F$ is any linear combination of the Bessel functions $J_0$ and $Y_0$.

\vskip 12pt

Any finite linear combination of functions of the form \eqref{generalized solution} also solves \eqref{schrodinger}.

\subsection*{Solution Method}

% I used a Sage-based computer program with the following ansatz.

I found this solution roughly as follows.\footnote{
I discovered an alternate form of this solution using a somewhat more complex ansatz
on January 24, 2023.  By January 26, I had established the solution in its current form.
The original ansatz produced a rational function with
a 1254 term numerator and a 36 term denominator, that gave rise to a system of 224 equations.
}

Use Cartesian coordinates.  Let $v$ be a linear polynomial in the coordinates and the root $r=\sqrt{x^2+y^2+z^2}$,
with the following form (the $v_i$ are constants):

\begin{equation}
\label{v ansatz}
v = v_0 r + v_1 x + v_2 y + v_3 z
\end{equation}

Assume the solution to the input PDE \eqref{schrodinger}
is a linear second-order ODE w.r.t. $v$ with linear coefficeints,
with the following form:

\begin{equation}
\label{psi ansatz}
(d_0 + d_1 v) \frac{\delta^2\Psi}{\delta v^2} - (m_0 + m_1 v) \frac{\delta\Psi}{\delta v} - (n_0 + n_1 v) \Psi = 0
\end{equation}

or:

\begin{equation}
\label{psi ansatz sub}
(d_0 + d_1 v) \frac{\delta^2\Psi}{\delta v^2} = (m_0 + m_1 v) \frac{\delta\Psi}{\delta v} + (n_0 + n_1 v) \Psi
\end{equation}

Substituting \eqref{v ansatz} into \eqref{psi ansatz sub}, expanding derivatives in \eqref{schrodinger},
substituting the RHS of \eqref{psi ansatz sub} into \eqref{schrodinger} where the LHS of
\eqref{psi ansatz sub} appears,
replacing all instances of $r^2$ with $x^2+y^2+z^2$, and canceling GCDs, we obtain a rational function
with a 228 term numerator and an 18 term denominator.  We ignore the denominator.  The numerator begins:

\begin{equation}
%% -8r\Psi x^5 b_1 b_2^2 n_1 - r\Psi x^5 b_1 b_4^2 n_1 - r \Psi x^5 b_1 b_6^2 n_1 - 2r\Psi x^5 b_2 b_3 b_4 n_1 - \cdots
-2r\Psi x^3 E d_1 v_1 - 3r\Psi x^3 n_1 v_0^2 v_1 - r\Psi x^3 n_1 v_1^3 - r\Psi x^3 n_1 v_1 v_2^2 - r\Psi x^3 n_1 v_1 v_3^2 - \cdots
\end{equation}

We collect like terms in $x$, $y$, $z$, $r$, $\Psi$, and $\Psi'$, organizing the numerator like this:

\begin{equation}
%%\left(-8 b_1 b_2^2 n_1 - b_1 b_4^2 n_1 - b_1 b_6^2 n_1 - 2 b_2 b_3 b_4 n_1 - 2b_2 b_5 b_6 n_1 \right) r\Psi x^5 - \cdots
r\Psi x^3 \left(-2 E d_1 v_1 - 3 n_1 v_0^2 v_1 - n_1 v_1^3 - n_1 v_1 v_2^2 - n_1 v_1 v_3^2\right) - \cdots
\end{equation}

The expressions in parenthesis gives us a system of equations (only one is shown)
involving the $v_i$, $d_i$, $m_i$ and $n_i$ variables that, if satisfied,
will yield a solution to \eqref{schrodinger} in the form \eqref{v ansatz} and \eqref{psi ansatz}.  Once
duplicate equations are dropped, the system has 34 equations.


%%sage: latex(matrix(eqns()).transpose())
\begin{equation}
\label{polynomial system}
\left(\begin{array}{r}
-2.0 \, n_{1} v_{0}^{3} - 4.0 \, n_{1} v_{0} v_{1}^{2} - 4.0 \, n_{1} v_{0} v_{2}^{2} - 2.0 \, n_{1} v_{0} v_{3}^{2} - 4.0 \, E d_{1} v_{0} \\
-2.0 \, n_{1} v_{0}^{3} - 4.0 \, n_{1} v_{0} v_{1}^{2} - 2.0 \, n_{1} v_{0} v_{2}^{2} - 4.0 \, n_{1} v_{0} v_{3}^{2} - 4.0 \, E d_{1} v_{0} \\
-2.0 \, n_{1} v_{0}^{3} - 2.0 \, n_{1} v_{0} v_{1}^{2} - 4.0 \, n_{1} v_{0} v_{2}^{2} - 4.0 \, n_{1} v_{0} v_{3}^{2} - 4.0 \, E d_{1} v_{0} \\
-4.0 \, m_{1} v_{0} v_{1} v_{2} \\
-4.0 \, m_{1} v_{0} v_{1} v_{3} \\
-4.0 \, m_{1} v_{0} v_{2} v_{3} \\
-4.0 \, n_{1} v_{0} v_{1} v_{2} \\
-4.0 \, n_{1} v_{0} v_{1} v_{3} \\
-4.0 \, n_{1} v_{0} v_{2} v_{3} \\
-3.0 \, m_{1} v_{0}^{2} v_{1} - 1.0 \, m_{1} v_{1}^{3} - 1.0 \, m_{1} v_{1} v_{2}^{2} - 1.0 \, m_{1} v_{1} v_{3}^{2} \\
-3.0 \, m_{1} v_{0}^{2} v_{2} - 1.0 \, m_{1} v_{1}^{2} v_{2} - 1.0 \, m_{1} v_{2}^{3} - 1.0 \, m_{1} v_{2} v_{3}^{2} \\
-3.0 \, m_{1} v_{0}^{2} v_{3} - 1.0 \, m_{1} v_{1}^{2} v_{3} - 1.0 \, m_{1} v_{2}^{2} v_{3} - 1.0 \, m_{1} v_{3}^{3} \\
-2.0 \, m_{1} v_{0}^{3} - 4.0 \, m_{1} v_{0} v_{1}^{2} - 4.0 \, m_{1} v_{0} v_{2}^{2} - 2.0 \, m_{1} v_{0} v_{3}^{2} \\
-2.0 \, m_{1} v_{0}^{3} - 4.0 \, m_{1} v_{0} v_{1}^{2} - 2.0 \, m_{1} v_{0} v_{2}^{2} - 4.0 \, m_{1} v_{0} v_{3}^{2} \\
-3.0 \, n_{1} v_{0}^{2} v_{1} - 1.0 \, n_{1} v_{1}^{3} - 1.0 \, n_{1} v_{1} v_{2}^{2} - 1.0 \, n_{1} v_{1} v_{3}^{2} - 2.0 \, E d_{1} v_{1} \\
-3.0 \, n_{1} v_{0}^{2} v_{2} - 1.0 \, n_{1} v_{1}^{2} v_{2} - 1.0 \, n_{1} v_{2}^{3} - 1.0 \, n_{1} v_{2} v_{3}^{2} - 2.0 \, E d_{1} v_{2} \\
-3.0 \, n_{1} v_{0}^{2} v_{3} - 1.0 \, n_{1} v_{1}^{2} v_{3} - 1.0 \, n_{1} v_{2}^{2} v_{3} - 1.0 \, n_{1} v_{3}^{3} - 2.0 \, E d_{1} v_{3} \\
-2.0 \, m_{1} v_{0}^{3} - 2.0 \, m_{1} v_{0} v_{1}^{2} - 4.0 \, m_{1} v_{0} v_{2}^{2} - 4.0 \, m_{1} v_{0} v_{3}^{2} \\
-1.0 \, n_{0} v_{0}^{2} - 1.0 \, n_{0} v_{1}^{2} - 1.0 \, n_{0} v_{2}^{2} - 1.0 \, n_{0} v_{3}^{2} - 2.0 \, E d_{0} - 2.0 \, d_{1} v_{0} \\
-2.0 \, n_{0} v_{0} v_{1} - 2.0 \, d_{1} v_{1} \\
-2.0 \, n_{0} v_{0} v_{2} - 2.0 \, d_{1} v_{2} \\
-2.0 \, n_{0} v_{0} v_{3} - 2.0 \, d_{1} v_{3} \\
-2.0 \, d_{1} v_{0} v_{1} - 2.0 \, m_{0} v_{0} v_{1} \\
-2.0 \, d_{1} v_{0} v_{2} - 2.0 \, m_{0} v_{0} v_{2} \\
-2.0 \, d_{1} v_{0} v_{3} - 2.0 \, m_{0} v_{0} v_{3} \\
-1.0 \, n_{1} v_{0}^{3} - 3.0 \, n_{1} v_{0} v_{1}^{2} - 1.0 \, n_{1} v_{0} v_{2}^{2} - 1.0 \, n_{1} v_{0} v_{3}^{2} - 2.0 \, E d_{1} v_{0} \\
-1.0 \, n_{1} v_{0}^{3} - 1.0 \, n_{1} v_{0} v_{1}^{2} - 3.0 \, n_{1} v_{0} v_{2}^{2} - 1.0 \, n_{1} v_{0} v_{3}^{2} - 2.0 \, E d_{1} v_{0} \\
-1.0 \, n_{1} v_{0}^{3} - 1.0 \, n_{1} v_{0} v_{1}^{2} - 1.0 \, n_{1} v_{0} v_{2}^{2} - 3.0 \, n_{1} v_{0} v_{3}^{2} - 2.0 \, E d_{1} v_{0} \\
-2.0 \, d_{1} v_{0}^{2} - 1.0 \, m_{0} v_{0}^{2} - 1.0 \, m_{0} v_{1}^{2} - 1.0 \, m_{0} v_{2}^{2} - 1.0 \, m_{0} v_{3}^{2} \\
-2.0 \, d_{0} \\
-2.0 \, d_{0} v_{0} \\
-1.0 \, m_{1} v_{0}^{3} - 3.0 \, m_{1} v_{0} v_{1}^{2} - 1.0 \, m_{1} v_{0} v_{2}^{2} - 1.0 \, m_{1} v_{0} v_{3}^{2} \\
-1.0 \, m_{1} v_{0}^{3} - 1.0 \, m_{1} v_{0} v_{1}^{2} - 3.0 \, m_{1} v_{0} v_{2}^{2} - 1.0 \, m_{1} v_{0} v_{3}^{2} \\
-1.0 \, m_{1} v_{0}^{3} - 1.0 \, m_{1} v_{0} v_{1}^{2} - 1.0 \, m_{1} v_{0} v_{2}^{2} - 3.0 \, m_{1} v_{0} v_{3}^{2}
\end{array}\right)
\end{equation}

Several solution techniques are available to solve a system of polynomial equations; I used
a numerical approximation technique.  A laptop computer finds the following approximate solution
in less than three seconds:

\begin{equation}
\label{witness point}
\addtolength{\jot}{-3pt}
\begin{aligned}
&\verb!(E, 1.2793593235207163e-32)!\\
&\verb!(d0, 1.4231937528298923e-43)!\\
&\verb!(d1, 1.0)!\\
&\verb!(m0, -1.0)!\\
&\verb!(m1, 5.0839108285704363e-42)!\\
&\verb!(n0, -1.0)!\\
&\verb!(n1, -8.795561312674161e-33)!\\
&\verb!(v0, 1.0)!\\
&\verb!(v1, 0.47255672374941904)!\\
&\verb!(v2, 0.5379975369878418)!\\
&\verb!(v3, 0.6980320859632679)!
\end{aligned}
\end{equation}

This is a {\it witness point}, a term common in the literature, an approximate solution accurate enough
to recover an exact solution.

In this case, exactness recovery is simple and straightforward.  $E$, $d_0$, $m_1$ and $n_1$ are all quite small,
so we set them to zero, while $d_1$, $m_0$, $n_0$ and $v_0$ already have their exact values and:

\begin{equation*}
0.4725567237^2 + 0.5379975369^2 + 0.6980320859^2 \approx  1.0000000000
\end{equation*}

so we add $v_1^2 + v_2^2 + v_3^2 = 1$ to our solution and conclude that our witness point lies approximately
on the following algebraic variety:

\begin{equation}
\begin{gathered}
E = 0 \\
d_0 = 0 \qquad
d_1 = 1 \\
m_0 = -1 \qquad
m_1 = 0 \\
n_0 = -1 \qquad
n_1 = 0 \\
v_0 = 1 \qquad
v_1^2 + v_2^2 + v_3^2 = 1
\end{gathered}
\end{equation}

Substituting these values back into our ansatz, we conclude that $\Psi(v)$
is a solution of \eqref{schrodinger} under these conditions:

\begin{equation}
\label{related solution}
\begin{gathered}
v \frac{\delta^2\Psi}{\delta v^2} + \frac{\delta\Psi}{\delta v} + \Psi = 0 \\
v = v_1 x+ v_2 y+ v_3 z+r \\
v_1^2 + v_2^2 + v_3^2 = 1
\end{gathered}
\end{equation}

We now have to solve a second order ODE:

\begin{equation}
v \Psi''(v) + \Psi'(v) + \Psi(v) = 0
\end{equation}

Wolfram Mathematica\footnote{I originally used Wolfram Alpha} can now
analyze this equation and determine that it is equivalent
to the Bessel function \eqref{solution}.

\includegraphics[page=1, clip, trim=1in 9in 1in 1in, width=\textwidth]{find Bessel solution.pdf}

\subsection*{Software}

The program used to find the witness point is available here:

\centerline{\url{https://github.com/BrentBaccala/helium}}

It's a Sage script that works fine with Sage 9.0 on Ubuntu 20.

Use it to find the witness point \eqref{witness point} by
running Sage as follows:

\begin{verbatim}
load('helium.sage')     # loads the script
prep_hydrogen(5)        # select PDE:hydrogen and ansatz:5
multi_init()            # form the equation
multi_expand()          # expand out the numerator and collect like terms into a matrix
random_numerical()      # run numerical optimizer
\end{verbatim}

Here are some other convenient variables and functions in the script:

\begin{verbatim}
V, D, N, M              # trial forms of various polynomials
eq_a                    # PDE with ansatz substituted in and r^2 simplified
R                       # polynomial ring over integers
F                       # fraction field of R
F_eq_a                  # eq_a expanded out (in F)
F_eq_a_n                # expanded numerator (in R)
F_eq_a_d                # expanded denominator (in R)
eqns()                  # system of equations to solve
SciMin                  # solution from scipy.optimize.root
\end{verbatim}

\subsection*{Exact Methods}

I used a numerical algorithm to solve the system of equations in expectation
of it being faster for larger systems, but the system \eqref{polynomial system}
is small enough that exact methods are usable.  In particular, we can form
an ideal $I$ in $\mathbf{Q}[v_0,...,m_1,E]$ from \eqref{polynomial system}.
Sage can calculate a Gr\"obner basis for the radical $I$ in less than a second.
While we could work with the Gr\"obner basis directly\footnote{A Gr\"obner basis
of the solution ideal in lexicographic order $E>d_i>m_i>n_i>v_i$ contains 65 polynomials.}
, I find it more
useful to study the primary decomposition, which Sage computes using
the Shimoyama-Yokoyama algorithm\footnote{Localization and Primary Decomposition of
Polynomial Ideals, {\it J. Symbolic Computation} (1996) {\bf 22}, 247–277}
as implemented in Singular.  Gr\"obner basis calculations are done
as a subalgorithm of Shimoyama-Yokoyama.

\begin{subequations}
%% sage: list(map(latex, I.radical().primary_decomposition()))
%% \Bold.* -> \\
\begin{align}
& \left(v_{0}^{2} - v_{1}^{2} - v_{2}^{2} - v_{3}^{2}, n_{1}, m_{1}, m_{0} - n_{0} v_{0}, d_{1} + n_{0} v_{0}, d_{0}, E\right)\label{ideal:1} \\
& \left(v_{3}, v_{2}, v_{1}, m_{1}, m_{0} - n_{0} v_{0}, 2 d_{1} + n_{0} v_{0}, d_{0}, E n_{0} - n_{1} v_{0}\right)\label{ideal:2}\\
& \left(v_{1}^{2} + v_{2}^{2} + v_{3}^{2}, v_{0}, d_{1}, d_{0}\right)\label{ideal:3}\\
& \left(v_{3}, v_{2}, v_{1}, v_{0}, d_{0}\right)\label{ideal:4}\\
& \left(n_{1}, n_{0}, m_{1}, m_{0}, d_{1}, d_{0}\right)\label{ideal:5}
\end{align}
\end{subequations}

Several of these varieties solve the system of equations, but do not lead to a meaningful solution to the differential equation.
In brief,

\begin{itemize}
\item[\eqref{ideal:1}] contains our solution and simple variants of it;
\item[\eqref{ideal:2}] contains more solutions, and is even more difficult to understand that \eqref{ideal:1};
\item[\eqref{ideal:3}] sets the coefficient of
%$\frac{d^2\Psi}{dv^2}$
$D^2\Psi$
in the ODE to zero, resulting in a first-order ODE, so we discard it;
\item[\eqref{ideal:4}] sets the variable $v$ to zero, so we discard it;
\item[\eqref{ideal:5}] sets all coefficients of the ODE to zero, so we discard it, too.

\end{itemize}


Ideal \eqref{ideal:1} contains our solution.  Does it contain any additional solutions?  $d_0$ is an ideal generator,
so $d_0=0$, and we can always multiply our DE by a constant without affecting our result, so we can set $d_1=1$.
Likewise, we can multiply our variable $v$ by a constant and that will only change our coefficients by constants.
Our coordinates are in real space, so our $v_i$ coefficients must be real (why?), so all of their squares must
be positive, and since $v_0^2-v_1^2-v_2^2-v_3^3=0$, $v_0$ must be non-zero.  So we can normalize by setting $v_0=1$.
That simplifies \eqref{ideal:1} to this:

\begin{equation}
\left(v_{1}^{2} + v_{2}^{2} + v_{3}^{2} - 1, v_{0} - 1, n_{1}, n_{0} + 1, m_{1}, m_{0} + 1, d_{1} - 1, d_{0}, E\right)
\end{equation}

So we conclude that \eqref{ideal:1} contains only our solution and simple variants thereof.

Ideal \eqref{ideal:2} is also interesting.  Setting $d_1=1$ and $v_0=1$ (same logic as above), it simplies to:

\begin{equation}
\left(v_{3}, v_{2}, v_{1}, v_{0} - 1, n_{0} + 2, m_{1}, m_{0} + 2, d_{1} - 1, d_{0}, 2 E + n_{1}\right)
\end{equation}

\begin{equation}
\begin{gathered}
v=r \\
v \Psi'' + 2 \Psi' + 2(1 + E v) \Psi = 0
\end{gathered}
\end{equation}

A good question to ask is if any of these solutions are in $L^2$.  Remember that these are radial solutions,
so we expect to find our classical solutions in the bunch!

Mathematica finds solutions using hypergeometric series:

\includegraphics[page=1, clip, trim=1in 8.5in 1in 1in, width=\textwidth]{ideal 2.pdf}

{\tt Hypergeometric1F1} and {\tt HypergeometricU} are ${}_1F_1(a;b;z)$ and $U(a;b;z)$,
the confluent hypergeometric functions of the first and second kind,
respectively.  Both are solutions to Kummer's equation:

\begin{equation}
z\frac{d^2w}{dz^2} + (b-z)\frac{dw}{dz} - aw = 0
\end{equation}

Kummer's solution to this equation is ${}_1F_1(a;b;z)$:

\begin{equation}
{}_1F_1(a;b;z) = \sum_{n=0}^\infty \frac{a^{(n)}z^n}{b^{(n)}n!}
\end{equation}

(n) is the rising factorial.

Since Kummer's equation is a second-order linear ODE, we expect two linearly independent
solutions.  Tricomi's solution $U(a;b;z)$ not only solves Kummer's equation, but
for many values of $a$ and $b$, is linearly independent of Kummer's solution:

\begin{equation}
U(a;b;z) = \frac{\Gamma(1-b)}{\Gamma(a+1-b)} {}_1F_1(a;b;z) + \frac{\Gamma(b-1)}{\Gamma(a)} z^{1-b} {}_1F_1(a-1-b;2-b;z)
\end{equation}

Their presence here isn't too surprising.  A quick skim of Kunwar and van Hoeij,
{\it Second Order Differential Equations with Hypergeometric Solutions of Degree Three}
\url{https://www.math.fsu.edu/~hoeij/papers/issac13/2.pdf}
left me with the impression that many, but by no means all, second-order linear ODEs have
solutions with hypergeometric functions.

The major advantage possessed by all of these solutions, indeed the reason they are sought,
is that they are linear ODEs that solve a PDE.  In many ways, a hypergeometric series
is an even better form for the solution than a linear ODE.
We no longer need to use the second-order differential equation
form at all, as a series expansion is now available.  What are its convergence properties?
According to Mathematica public docs, it can be approximated to any precision.

I checked all energy levels from -10 to 10 in steps of 1/8.  For only these four did
Mathematica find any special simplification of the hypergeometric series:

\includegraphics[page=1, clip, trim=1in 5.75in 1in 3.25in, width=\textwidth]{ideal 2.pdf}

We see our classical solutions of $e^{-r}$ for energy $-\frac{1}{2}$ and $(2-r)e^{-r/2}$
for energy $-\frac{1}{8}$  How did we get $e^{-2r}$ for $-2$?

Why is $e^{-r}$ paired with some weird Ei integral?

The pattern: -1/2 maps to $e^{-r}$; -1/8 maps to $e^{-r/2}$; -1/18 maps to $e^{-r/3}$;
generalized as E maps to $e^{-\sqrt{2E}r}$.

The corresponding differential operator is $D+\sqrt{-2E}I$.  Squaring this we get $D^2 + 2\sqrt{-2E}D - 2E$.
This operator contains a $\sqrt{E}$ in the coefficient of the first-order term.  So we can't construct
a second-order differential operator that gives us $e^{-\sqrt{-2E}r}$ with multiplicity two unless we
allow $\sqrt{E}$ into our coefficients, which this ansatz does not.

We {\it can} however, pair $D+\sqrt{-2E}I$ with a different solution and obtain a suitable operator
that gives us the solution pair found by Mathematica.

Factoring in the Weyl algebra, we obtain:

\begin{equation}
vD^2 + 2D + (2+2Ev) I = (vD + (2-\sqrt{-2E}v)I) (D+\sqrt{-2E}I) + (2-2\sqrt{-2E})I
\end{equation}

so the factorization is only exact if E=-1/2.  In that case:

\begin{equation}
\label{Weyl algebra}
vD^2 + 2D + (2-v) I = (vD + (2-v)I) (D+I)
\end{equation}

Makes sense because only in the E=-1/2 case does a simple exponential solve the PDE.  In
higher energy cases we have to multiply by a Laguerre function like $(2-r)$.

Why isn't $(D+I)^2 = (D^2+2D+I)$ represented in our solution ideal?

Traditional methods\footnote{\url{https://dec41.user.srcf.net/h/IA_M/differential_equations/5_1}}
of solving second-order linear ODEs with constant coefficients tell us that degenerate
solutions like this yield both $e^{-r}$ and $re^{-r}$ as solutions.

So, we've introduced a new solution $y=re^{-r}$ that satisfies $Dy=e^{-r}-re^{-r}=(1-r)y$.

Is $D+(v-1)I$ a divisor of \eqref{Weyl algebra}?  How?

\subsection*{The Rosenfeld-Gr\"obner algorithm}

\subsection*{The Ansatzen}

These are current and future ansatzen for the form of the ODE:

For hydrogen, the PDE is always $\nabla^2 \Psi - \frac{1}{r} \Psi = E \Psi$

\begin{itemize}
\item[Ansatz 1:] Exponential of a first degree polynomial times a first degree polynomial

Expected to find 1s and 2s levels of hydrogen, $e^{-r}$ and $(2-r)e^{r/2}$

$A,B \in \mathbf{C}[x,y,z,r]; \deg A \le 1; \deg B \le 1$

$\Psi = A F; F' = F; F_x = B_x F'; F_y = B_y F'; F_z = B_z F'$

or just: $\Psi = A F; F_x = B_x F; F_y = B_y F; F_z = B_z F$

Example: $B=-r; A=1; E=-1/2$ $\qquad\Rightarrow\qquad$ $e^{-r}$

Example: $B=-r/2; A=r-2; E=-1/8$ $\qquad\Rightarrow\qquad$ $(2-r)e^{-r/2}$

\item[Ansatz 2:] Logarithm of a first degree polynomial times a first degree polynomial

$A,B \in \mathbf{C}[x,y,z,r]; \deg A \le 1; \deg B \le 1$

$\Psi = A F; F' = 1/F; F_x = B_x F'; F_y = B_y F'; F_z = B_z F'$

or just: $\Psi = A F; F_x = B_x / F; F_y = B_y / F; F_z = B_z / F$

\item[Ansatz 3,4:] Unused

\item[Ansatz 5:] Second-order ODE with linear coefficients and a linear variable

Expected to find 1s level of hydrogen, $e^{-r}$

$A,B,C,V \in \mathbf{C}[x,y,z,r]; \deg A \le 1; \deg B \le 1; \deg C \le 1; \deg V \le 1$

$\Psi = F; A F'' + B F' + C F = 0; F_x = V_x F'; F_y = V_y F'; F_z = V_z F'$

Example: $V=-r; A+C=-B; E=-1/2$ ($A, B, C \in \mathbf{C}$) $\Rightarrow$ $e^{-r}$

\item[Ansatz 6:] Second-order ODE with linear coefficients and a first-degree rational function variable

\item[Ansatz 7:] Second-order ODE with second-degree coefficients and a second-degree rational function variable

\item[Ansatz 8:] (logically before ansatz 5) First-order ODE with linear coefficients and a linear polynomial variable

\item[Ansatz 9:] (logically before ansatz 8) First-order ODE with constant coefficients and a linear polynomial variable

\item[Ansatz 10:] (logically before ansatz 7) Second-order ODE with second-degree coefficients and a second-degree polynomial variable

\item[Ansatz 11:] An second-degree algebraic extension with second-degree coefficients (involving $E$?),
followed by ansatz 7 (second-order ODE with second-degree coefficients and a second-degree rational function variable)

\item[Ansatz 12:] Two nested second-order ODEs with second-degree coefficients and a second-degree rational function variable (ansatz 7 twice)

\item[Ansatz 13:] Ansatz 11 twice

\end{itemize}

\subsection*{Draft Status}

This paper is still a draft and is being updated regularly.

\subsection*{Contact}

The author maintains a discussion page for this result on his personal blog at:

\begin{center}
\small
\url{https://www.freesoft.org/blogs/soapbox/a-new-solution-of-hydrogen/}
\end{center}

\end{document}
